{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jFFRbCqzRY8"
      },
      "source": [
        "In this ICP we will build 2 feed forward network.\n",
        "\n",
        ">Auto Encoder\n",
        "\n",
        ">CNN\n",
        "\n",
        ">Data Set\n",
        "\n",
        ">>The MNIST database of handwritten digits. Training set of 60,000 examples, and a test set of 10,000 examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGzIpl7P0U6_"
      },
      "source": [
        "Auto Encoders:\n",
        "\n",
        ">>Data compression is a big topic that’s used in computer vision, computer networks, computer architecture, and many other fields.The point of data compression is to convert our input into a smaller representation that we recreate, to a degree of quality. This smaller representation is what would be passed around, and, when anyone needed the original, they would reconstruct it from the smaller representation.Autoencoders are unsupervised neural networks that use machine learning to do this compression \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9_cWbPki6tv"
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        " \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZMxXfcA0wIW"
      },
      "source": [
        "Load  MNIST handwritten digits dataset.\n",
        "\n",
        "There is no need to load labels because autoencoders are unsupervised.\n",
        "\n",
        "Rescale our images from 0 – 255 to 0 – 1 and flatten them out.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z3fLLrtjO1X"
      },
      "source": [
        "#Notice that we’re not loading any of the labels because autoencoders are unsupervised\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "# rescale our images from 0 – 255 to 0 – 1 and flatten them out.\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MceFF_0BjO9O",
        "outputId": "8232cf7a-2f67-4d4e-d79e-fd03ca87e164"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BupxtkW-jPDk",
        "outputId": "b85e2ff0-f6cc-452d-c9c2-69504cb10ad9"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tzUyrr7rBUg"
      },
      "source": [
        "INPUT_SIZE = 784\n",
        "ENCODING_SIZE = 64"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h3DN2WW1Hjn"
      },
      "source": [
        "Create autoencoder\n",
        "\t\n",
        "Use ReLU  activation functions \n",
        "\n",
        "create constants for our input size and our encoding size. \n",
        "\n",
        "reduce our input from 784 -> 512 -> 256 -> 128 -> 64, encoder path\n",
        "\n",
        "then expand it back up 64 -> 128 -> 256 -> 512 -> 784, decoder path\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oacp6YGKjhtR"
      },
      "source": [
        "#reduce our input from 784 -> 512 -> 256 -> 128 -> 64 (encoder path), then expand it back up 64 -> 128 -> 256 -> 512 -> 784(decoder path).\n",
        "# Also notice the relu activation function\n",
        "input_img = Input(shape=(INPUT_SIZE,))\n",
        "encoded = Dense(512, activation='relu')(input_img)\n",
        "encoded = Dense(256, activation='relu')(encoded)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dense(ENCODING_SIZE, activation='relu')(encoded)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSF9HPsAj2ns"
      },
      "source": [
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoded = Dense(512, activation='relu')(decoded)\n",
        "decoded = Dense(INPUT_SIZE, activation='relu')(decoded)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tMXFCw5j9lB"
      },
      "source": [
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2awwn1Hw11C-"
      },
      "source": [
        "Training the model and then making the prediction \n",
        "\n",
        "Build and train the model.\n",
        "\n",
        "Use the ADAM optimizer and mean squared error loss (the Euclidean distance/loss) between the input and reconstruction\n",
        "\n",
        "encode and decode the test set to see how well model is performing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ0OekOij_hR",
        "outputId": "042d94c9-4abb-4ec0-9c1e-8dc64af85867"
      },
      "source": [
        "#using ADAM optimizer and mean squared error loss (the Euclidean distance/loss) between the input and reconstruction\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error',)\n",
        "autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "188/188 [==============================] - 2s 6ms/step - loss: 0.0559 - val_loss: 0.0206\n",
            "Epoch 2/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0189 - val_loss: 0.0151\n",
            "Epoch 3/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0146 - val_loss: 0.0132\n",
            "Epoch 4/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0128 - val_loss: 0.0120\n",
            "Epoch 5/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0118 - val_loss: 0.0114\n",
            "Epoch 6/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0110 - val_loss: 0.0108\n",
            "Epoch 7/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0105 - val_loss: 0.0103\n",
            "Epoch 8/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0100 - val_loss: 0.0101\n",
            "Epoch 9/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0096 - val_loss: 0.0099\n",
            "Epoch 10/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0094 - val_loss: 0.0095\n",
            "Epoch 11/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0091 - val_loss: 0.0091\n",
            "Epoch 12/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0089 - val_loss: 0.0091\n",
            "Epoch 13/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0087 - val_loss: 0.0088\n",
            "Epoch 14/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0089\n",
            "Epoch 15/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0084 - val_loss: 0.0086\n",
            "Epoch 16/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0083 - val_loss: 0.0084\n",
            "Epoch 17/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0082 - val_loss: 0.0084\n",
            "Epoch 18/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0081 - val_loss: 0.0083\n",
            "Epoch 19/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0080 - val_loss: 0.0082\n",
            "Epoch 20/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0079 - val_loss: 0.0082\n",
            "Epoch 21/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0078 - val_loss: 0.0079\n",
            "Epoch 22/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0077 - val_loss: 0.0082\n",
            "Epoch 23/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0076 - val_loss: 0.0079\n",
            "Epoch 24/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0078\n",
            "Epoch 25/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0078\n",
            "Epoch 26/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0076\n",
            "Epoch 27/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0078\n",
            "Epoch 28/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0075\n",
            "Epoch 29/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0075\n",
            "Epoch 30/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0072 - val_loss: 0.0076\n",
            "Epoch 31/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0072 - val_loss: 0.0074\n",
            "Epoch 32/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0070 - val_loss: 0.0076\n",
            "Epoch 33/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0071 - val_loss: 0.0073\n",
            "Epoch 34/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0070 - val_loss: 0.0074\n",
            "Epoch 35/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0070 - val_loss: 0.0071\n",
            "Epoch 36/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0069 - val_loss: 0.0073\n",
            "Epoch 37/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0069 - val_loss: 0.0073\n",
            "Epoch 38/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0069 - val_loss: 0.0072\n",
            "Epoch 39/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0069 - val_loss: 0.0071\n",
            "Epoch 40/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0068 - val_loss: 0.0074\n",
            "Epoch 41/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0068 - val_loss: 0.0072\n",
            "Epoch 42/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0067 - val_loss: 0.0070\n",
            "Epoch 43/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0067 - val_loss: 0.0071\n",
            "Epoch 44/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0067 - val_loss: 0.0070\n",
            "Epoch 45/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0067 - val_loss: 0.0070\n",
            "Epoch 46/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0069\n",
            "Epoch 47/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0068\n",
            "Epoch 48/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0069\n",
            "Epoch 49/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0068\n",
            "Epoch 50/50\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0064 - val_loss: 0.0068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6150243d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmLdgFuLkHLE"
      },
      "source": [
        "#After our autoencoder has trained, we can try to encode and decode the test set to see how well our autoencoder can compress\n",
        "decoded_imgs = autoencoder.predict(X_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9AV9WA32DO4"
      },
      "source": [
        "Visvualize the results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "4QGe9UvVu2gL",
        "outputId": "5810f9d2-e409-4a09-e984-a392c8278419"
      },
      "source": [
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(10):\n",
        "    # original\n",
        "    plt.subplot(2, 10, i + 1)\n",
        "    plt.imshow(X_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    plt.axis('off')\n",
        " \n",
        "    # reconstruction\n",
        "    plt.subplot(2, 10, i + 1 + 10)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    plt.axis('off')\n",
        " \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPuU-xlr2IQi"
      },
      "source": [
        "CNN \n",
        "\n",
        "The Problem: MNIST digit classification\n",
        "\t\n",
        "We’re going to tackle a classic introductory Computer Vision problem: MNIST handwritten digit classification. It’s simple: given an image, classify it as a digit\n",
        "\t\n",
        "Each image in the MNIST dataset is 28x28 and contains a centered, grayscale digit. Our CNN will take an image and output one of 10 possible classes (one for each digit)\n",
        "\n",
        "First of all,  import all necessary packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAcah1drwp5F"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8wNQ_5JwrbI",
        "outputId": "3a0e9dcd-d549-4651-d606-af9c408faa1e"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sT7ApeR2u4I"
      },
      "source": [
        "Before we begin, we’ll normalize the image pixel values from [0, 255] to [-0.5, 0.5]\n",
        "\t \n",
        "This will  make our network easier to train (using smaller, centered values usually leads to better results). \n",
        "\n",
        "We’ll also reshape each image from (28, 28) to (28, 28, 1) because Keras requires the third dimension. (1 stands for color scheme, which is grey scale in this case). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fvE-hRywrlV"
      },
      "source": [
        "# Normalize the images.\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcah0Yovwro7"
      },
      "source": [
        "# Reshape the images.\n",
        "train_images = np.expand_dims(train_images, axis=3)\n",
        "test_images = np.expand_dims(test_images, axis=3)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73nP9Kv73IJt"
      },
      "source": [
        "Keras model is either built using the Sequential class, which represents a linear stack of layers, or the functional Model class, which is more customizable. \n",
        "\n",
        "We’ll be using the simpler Sequential model, since our CNN will be a linear stack of layers.\n",
        "\n",
        "num_filters, filter_size, and pool_size are self-explanatory variables that set the hyperparameters for our CNN.\n",
        "\n",
        "The first layer in any Sequential model must specify the input_shape, so we do so on Conv2D. \n",
        "\n",
        "Once this input shape is specified, Keras will automatically infer the shapes of inputs for later layers.\n",
        "\t\n",
        "The output Softmax layer has 10 nodes, one for each class\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soSpyDp6wrr4"
      },
      "source": [
        "\n",
        "num_filters = 8\n",
        "filter_size = 3\n",
        "pool_size = 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CpLEcMKw-CZ"
      },
      "source": [
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
        "  MaxPooling2D(pool_size=pool_size),\n",
        "  Flatten(),\n",
        "  Dense(10, activation='softmax'),\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkwwUCKQ4M7U"
      },
      "source": [
        "Before we can begin training, we need to configure the training process. We decide 3 key factors during the compilation step:\n",
        "\t\n",
        "The optimizer. We’ll stick with a pretty good default: the Adam gradient-based optimizer. Keras has many other optimizers you can look into as well.\n",
        "\n",
        "The loss function. Since we’re using a Softmax output layer, we’ll use the Cross-Entropy loss. Keras distinguishes between binary_crossentropy (2 classes) and categorical_crossentropy (>2 classes), so we’ll use the latter. \n",
        "\n",
        "A list of metrics. Since this is a classification problem, we’ll just have Keras report on the accuracy metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhflsBSvw-GX"
      },
      "source": [
        "# Compile the model.\n",
        "model.compile(\n",
        "  'adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVGOn2AX41Ub"
      },
      "source": [
        "Training a model in Keras literally consists only of calling fit() and specifying some parameters. There are a lot of possible parameters, but we’ll only supply these:\n",
        "\n",
        "The training data (images and labels), commonly known as X and Y, respectively.\n",
        "\n",
        "The number of epochs (iterations over the entire dataset) to train for.\n",
        "\n",
        "The validation data (or test data), which is used during training to periodically measure the network’s performance against data it hasn’t seen before.\n",
        "\n",
        "There’s one thing we have to be careful about:\n",
        "\n",
        "Keras expects the training targets to be 10-dimensional vectors, since there are 10 nodes in our Softmax output layer. Right now, our train_labels and test_labels arrays contain single integers representing the class for each image\n",
        "\n",
        "Conveniently, Keras has a utility method that fixes this exact issue:\n",
        "to_categorical: It turns our array of class integers into an array of one-hot vectors instead. For example, 2 would become [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] (it’s zero-indexed)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srpkxU0_w-JR",
        "outputId": "1a22232e-5bbb-493a-c069-fbfc45498a7d"
      },
      "source": [
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=30,\n",
        "  validation_data=(test_images, to_categorical(test_labels)),\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.5890 - accuracy: 0.8322 - val_loss: 0.2157 - val_accuracy: 0.9394\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.2037 - accuracy: 0.9405 - val_loss: 0.1497 - val_accuracy: 0.9570\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1387 - accuracy: 0.9597 - val_loss: 0.1113 - val_accuracy: 0.9673\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1092 - accuracy: 0.9679 - val_loss: 0.1059 - val_accuracy: 0.9675\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0939 - accuracy: 0.9719 - val_loss: 0.0898 - val_accuracy: 0.9718\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0787 - accuracy: 0.9768 - val_loss: 0.0875 - val_accuracy: 0.9710\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0729 - accuracy: 0.9778 - val_loss: 0.0834 - val_accuracy: 0.9745\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0690 - accuracy: 0.9790 - val_loss: 0.0800 - val_accuracy: 0.9741\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0622 - accuracy: 0.9818 - val_loss: 0.0798 - val_accuracy: 0.9736\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0566 - accuracy: 0.9829 - val_loss: 0.0769 - val_accuracy: 0.9753\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0534 - accuracy: 0.9844 - val_loss: 0.0831 - val_accuracy: 0.9731\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0494 - accuracy: 0.9853 - val_loss: 0.0785 - val_accuracy: 0.9760\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0467 - accuracy: 0.9856 - val_loss: 0.0818 - val_accuracy: 0.9750\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0469 - accuracy: 0.9860 - val_loss: 0.0842 - val_accuracy: 0.9743\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0803 - val_accuracy: 0.9756\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0400 - accuracy: 0.9872 - val_loss: 0.0769 - val_accuracy: 0.9761\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0377 - accuracy: 0.9885 - val_loss: 0.0803 - val_accuracy: 0.9752\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0375 - accuracy: 0.9882 - val_loss: 0.0879 - val_accuracy: 0.9743\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0365 - accuracy: 0.9889 - val_loss: 0.0841 - val_accuracy: 0.9761\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0343 - accuracy: 0.9897 - val_loss: 0.0868 - val_accuracy: 0.9753\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0354 - accuracy: 0.9897 - val_loss: 0.0877 - val_accuracy: 0.9749\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.0841 - val_accuracy: 0.9752\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0306 - accuracy: 0.9906 - val_loss: 0.0864 - val_accuracy: 0.9754\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0304 - accuracy: 0.9909 - val_loss: 0.0895 - val_accuracy: 0.9753\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0284 - accuracy: 0.9909 - val_loss: 0.0876 - val_accuracy: 0.9761\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0280 - accuracy: 0.9916 - val_loss: 0.1021 - val_accuracy: 0.9748\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.0944 - val_accuracy: 0.9743\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0253 - accuracy: 0.9919 - val_loss: 0.0884 - val_accuracy: 0.9754\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0239 - accuracy: 0.9929 - val_loss: 0.0937 - val_accuracy: 0.9746\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0234 - accuracy: 0.9933 - val_loss: 0.0931 - val_accuracy: 0.9759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdef780cb50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcMHM5u65_rF"
      },
      "source": [
        "We pass an array of inputs to predict() and it returns an array of outputs.\n",
        "\n",
        "The output of our network is 10 probabilities (because of softmax), so we’ll use np.argmax() to turn those into actual digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ganai2kyw-MX"
      },
      "source": [
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNqIPTlow-Ox",
        "outputId": "efb3c082-b2a8-4292-81a5-d57daafe8691"
      },
      "source": [
        "# Print our model's predictions.\n",
        "print(\"These are the model predictions :\")\n",
        "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
        "print(\"\\n\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "These are the model predictions :\n",
            "[7 2 1 0 4]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KAZx-dIw-SC",
        "outputId": "0ef8149c-7f83-4b07-9350-877b30a6eb41"
      },
      "source": [
        "# Check our predictions against the ground truths.\n",
        "print(\"These are the corresponding labels :\")\n",
        "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "These are the corresponding labels :\n",
            "[7 2 1 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}