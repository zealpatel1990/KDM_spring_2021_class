{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP-3_zeal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zealpatel1990/KDM_spring_2021_class/blob/main/ICP%203/source_code/ICP_3_zeal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YHmgOQDIhOQ"
      },
      "source": [
        "# Install stanza, Installing and importing Stanza are as simple as running the following commands. \r\n",
        "!pip install stanza\r\n",
        "\r\n",
        "# Import stanza\r\n",
        "import stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-HAmikAIvUl",
        "outputId": "f9e070b3-1c38-43ac-9a2a-44f06bd9843a"
      },
      "source": [
        "# Download the Stanford CoreNLP package with Stanza's installation command\r\n",
        "# This'll take several minutes, depending on the network speed\r\n",
        "corenlp_dir = './corenlp'\r\n",
        "stanza.install_corenlp(dir=corenlp_dir)\r\n",
        "\r\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\r\n",
        "import os\r\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-04 20:48:34 INFO: Installing CoreNLP package into ./corenlp...\n",
            "Downloading http://nlp.stanford.edu/software/stanford-corenlp-latest.zip: 100%|██████████| 505M/505M [02:24<00:00, 3.50MB/s]\n",
            "2021-02-04 20:51:01 WARNING: For customized installation location, please set the `CORENLP_HOME` environment variable to the location of the installation. In Unix, this is done with `export CORENLP_HOME=./corenlp`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDS6lHuCOITe",
        "outputId": "00659a97-ce04-45ce-ff2c-9f4df4555f41"
      },
      "source": [
        "import stanza\r\n",
        "# Import client module\r\n",
        "from stanza.server import CoreNLPClient\r\n",
        "\r\n",
        "client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \r\n",
        "endpoint='http://localhost:9003')\r\n",
        "client.start()\r\n",
        "import time\r\n",
        "time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-04 21:28:10 INFO: Writing properties to tmp file: corenlp_server-5443ac7c6cb1439d.props\n",
            "2021-02-04 21:28:10 INFO: Starting server with command: java -Xmx5G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9003 -timeout 150000000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-5443ac7c6cb1439d.props -annotators openie -preload -outputFormat serialized\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8aBxT8qLIaY",
        "outputId": "387583fc-9d2e-4975-ed45-749ed9fc2227"
      },
      "source": [
        "import json\r\n",
        "s='Twenty percent electric motors are pulled from an assembly line'\r\n",
        "s1='Brack Obama was born in Hawaii'\r\n",
        "text = \"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.\"\r\n",
        "document = client.annotate(s1, output_format='json')\r\n",
        "triples = []\r\n",
        "for sentence in document['sentences']:\r\n",
        "    for i in sentence:\r\n",
        "      for triple in sentence['openie']:\r\n",
        "        triples.append({\r\n",
        "           'subject': triple['subject'],\r\n",
        "           'relation': triple['relation'],\r\n",
        "            'object': triple['object']\r\n",
        "        })\r\n",
        "print(triples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'subject': 'Brack Obama', 'relation': 'was born in', 'object': 'Hawaii'}, {'subject': 'Brack Obama', 'relation': 'was', 'object': 'born'}, {'subject': 'Brack Obama', 'relation': 'was born in', 'object': 'Hawaii'}, {'subject': 'Brack Obama', 'relation': 'was', 'object': 'born'}, {'subject': 'Brack Obama', 'relation': 'was born in', 'object': 'Hawaii'}, {'subject': 'Brack Obama', 'relation': 'was', 'object': 'born'}, {'subject': 'Brack Obama', 'relation': 'was born in', 'object': 'Hawaii'}, {'subject': 'Brack Obama', 'relation': 'was', 'object': 'born'}, {'subject': 'Brack Obama', 'relation': 'was born in', 'object': 'Hawaii'}, {'subject': 'Brack Obama', 'relation': 'was', 'object': 'born'}, {'subject': 'Brack Obama', 'relation': 'was born in', 'object': 'Hawaii'}, {'subject': 'Brack Obama', 'relation': 'was', 'object': 'born'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiUu8HWcPcWs",
        "outputId": "15933c03-c238-4df9-c397-84f295598872"
      },
      "source": [
        "result = [document[\"sentences\"][0][\"openie\"] for item in document]\r\n",
        "for i in result:\r\n",
        "    for rel in i:\r\n",
        "        relationSent=rel['relation'],rel['subject'],rel['object']\r\n",
        "        print('The triplet of the given sentence is')\r\n",
        "        print(relationSent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The triplet of the given sentence is\n",
            "('was born in', 'Brack Obama', 'Hawaii')\n",
            "The triplet of the given sentence is\n",
            "('was', 'Brack Obama', 'born')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw4CS3WiQ0iC"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')\r\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6spDiWE7SN6P",
        "outputId": "96ea7662-417b-4853-b308-552491b89805"
      },
      "source": [
        "# lets use word paint as an exqmple\r\n",
        "syns = wordnet.synsets(\"good\")\r\n",
        "\r\n",
        "# An example of a synset:\r\n",
        "print(syns[0].name())\r\n",
        "print('\\n')\r\n",
        "# Just the word:\r\n",
        "print(syns[0].lemmas()[0].name())\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# Definition of that first synset:\r\n",
        "print(syns[0].definition())\r\n",
        "print('\\n')\r\n",
        "# Examples of the word in use in sentences:\r\n",
        "print(syns[0].examples())\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# synonyms and antonyms using wordnet using word\r\n",
        "synonyms = []\r\n",
        "antonyms = []\r\n",
        "\r\n",
        "for syn in wordnet.synsets(\"good\"):\r\n",
        "    for l in syn.lemmas():\r\n",
        "        synonyms.append(l.name())\r\n",
        "        if l.antonyms():\r\n",
        "            antonyms.append(l.antonyms()[0].name())\r\n",
        "print('The synonyms of good are: ')\r\n",
        "print(set(synonyms))\r\n",
        "print('\\n')\r\n",
        "print('The antonyms of good are: ')\r\n",
        "print(set(antonyms))\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# comparison/ similarity score between 2 words\r\n",
        "w1 = wordnet.synset('ship.n.01')\r\n",
        "w2 = wordnet.synset('boat.n.01') # n denotes noun\r\n",
        "print(\"The similarity score between ship and boat is =\",w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good.n.01\n",
            "\n",
            "\n",
            "good\n",
            "\n",
            "\n",
            "benefit\n",
            "\n",
            "\n",
            "['for your own good', \"what's the good of worrying?\"]\n",
            "\n",
            "\n",
            "The synonyms of good are: \n",
            "{'skilful', 'honorable', 'dear', 'respectable', 'practiced', 'dependable', 'sound', 'soundly', 'near', 'undecomposed', 'honest', 'unspoiled', 'proficient', 'estimable', 'commodity', 'secure', 'effective', 'salutary', 'beneficial', 'just', 'serious', 'goodness', 'ripe', 'expert', 'upright', 'in_effect', 'good', 'skillful', 'right', 'thoroughly', 'full', 'safe', 'well', 'unspoilt', 'trade_good', 'in_force', 'adept'}\n",
            "\n",
            "\n",
            "The antonyms of good are: \n",
            "{'ill', 'bad', 'evilness', 'evil', 'badness'}\n",
            "\n",
            "\n",
            "The similarity score between ship and boat is = 0.9090909090909091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smTzKpCBzcr-"
      },
      "source": [
        "# ICP Tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XD1RlppzjJj"
      },
      "source": [
        "### 1: IE: Triplet Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owYP5i1jVgyq"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.lang.en import English\r\n",
        "import networkx as nx\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# spliting input text in list of words\r\n",
        "def getSentences(text):\r\n",
        "    nlp = English()\r\n",
        "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\r\n",
        "    document = nlp(text)\r\n",
        "    return [sent.string.strip() for sent in document.sents]\r\n",
        "\r\n",
        "# Function to print word and token\r\n",
        "def printToken(token):\r\n",
        "    print(token.text, \"->\", token.dep_)\r\n",
        "\r\n",
        "def appendChunk(original, chunk):\r\n",
        "    return original + ' ' + chunk\r\n",
        "\r\n",
        "def isRelationCandidate(token):\r\n",
        "    deps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\r\n",
        "    return any(subs in token.dep_ for subs in deps)\r\n",
        "\r\n",
        "def isConstructionCandidate(token):\r\n",
        "    deps = [\"compound\", \"prep\", \"conj\", \"mod\"]\r\n",
        "    return any(subs in token.dep_ for subs in deps)\r\n",
        "\r\n",
        "# This function will pass subject, object, relation, subject construction and object contruction and print triplets of the sentence\r\n",
        "def processSubjectObjectPairs(tokens):\r\n",
        "    subject = ''\r\n",
        "    object = ''\r\n",
        "    relation = ''\r\n",
        "    subjectConstruction = ''\r\n",
        "    objectConstruction = ''\r\n",
        "    for token in tokens:\r\n",
        "        printToken(token)\r\n",
        "        if \"punct\" in token.dep_:\r\n",
        "            continue\r\n",
        "        if isRelationCandidate(token):\r\n",
        "            relation = appendChunk(relation, token.lemma_)\r\n",
        "        if isConstructionCandidate(token):\r\n",
        "            if subjectConstruction:\r\n",
        "                subjectConstruction = appendChunk(subjectConstruction, token.text)\r\n",
        "            if objectConstruction:\r\n",
        "                objectConstruction = appendChunk(objectConstruction, token.text)\r\n",
        "        if \"subj\" in token.dep_:\r\n",
        "            subject = appendChunk(subject, token.text)\r\n",
        "            subject = appendChunk(subjectConstruction, subject)\r\n",
        "            subjectConstruction = ''\r\n",
        "        if \"obj\" in token.dep_:\r\n",
        "            object = appendChunk(object, token.text)\r\n",
        "            object = appendChunk(objectConstruction, object)\r\n",
        "            objectConstruction = ''\r\n",
        "\r\n",
        "  # printing triplets of given sentence\r\n",
        "    print (\"\\nThe triplet of the given sentence is: \\nSubject: \", subject.strip(),\r\n",
        "            \",\\nRelation: \", relation.strip(),\r\n",
        "            \",\\nObject: \", object.strip())\r\n",
        "    return (subject.strip(), relation.strip(), object.strip())\r\n",
        "\r\n",
        "# This returns object pairs from the sentence by using NLP_model function\r\n",
        "def processSentence(sentence):\r\n",
        "    tokens = nlp_model(sentence)\r\n",
        "    return processSubjectObjectPairs(tokens)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CUvqMCQWHj7",
        "outputId": "36c588d8-f2db-4bae-f86b-d5acbfab55a7"
      },
      "source": [
        "\r\n",
        "#sample text\r\n",
        "text = \"Gandhiji was born in Porbandar.\"\r\n",
        "\r\n",
        "sentences = getSentences(text)\r\n",
        "nlp_model = spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "triples = []\r\n",
        "print (\"Input --> \", text)\r\n",
        "# printing the relation and construction candidate\r\n",
        "for sentence in sentences:\r\n",
        "    triples.append(processSentence(sentence))\r\n",
        "\r\n",
        "#Born is the past participle of the verb bear"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input -->  Gandhiji was born in Porbandar.\n",
            "Gandhiji -> nsubjpass\n",
            "was -> auxpass\n",
            "born -> ROOT\n",
            "in -> prep\n",
            "Porbandar -> pobj\n",
            ". -> punct\n",
            "\n",
            "The triplet of the given sentence is: \n",
            "Subject:  Gandhiji ,\n",
            "Relation:  bear ,\n",
            "Object:  Porbandar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdch2yWLzzRT"
      },
      "source": [
        "### 2: WordNet Task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTL3ux7fTyxT",
        "outputId": "88bb2543-9cea-4443-f386-77bd6236d137"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zai7br-Wz8U2"
      },
      "source": [
        "#### 1.Hyponym "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Rk_DxaTAdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4705e5d3-0d66-495b-e299-74c088f0df8b"
      },
      "source": [
        "vehicle_synset = wn.synset('vehicle.n.01')\r\n",
        "# getting sorted list of HYPONYMS from vehicle_synset\r\n",
        "vehicle_synset.hyponyms()\r\n",
        "# somethiing like child words"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('bumper_car.n.01'),\n",
              " Synset('craft.n.02'),\n",
              " Synset('military_vehicle.n.01'),\n",
              " Synset('rocket.n.01'),\n",
              " Synset('skibob.n.01'),\n",
              " Synset('sled.n.01'),\n",
              " Synset('steamroller.n.02'),\n",
              " Synset('wheeled_vehicle.n.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiQgy_O20h6L"
      },
      "source": [
        "#### 2.Hypernym "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5p5n8rlTBLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbca5510-9f62-4d6e-c1db-79d7fc389208"
      },
      "source": [
        "vehicle_synset = wn.synset('projectile.n.2')\r\n",
        "# getting sorted list of HYPONYMS from vehicle_synset\r\n",
        "sorted([lemma.name() for synset in vehicle_synset.hypernyms() for lemma in synset.lemmas()])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vehicle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b38sW-960mHM"
      },
      "source": [
        "#### 3.Meronym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHAj6XFiTBlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af1dc9fd-81b1-444f-ddd4-60cb30b8ed8f"
      },
      "source": [
        "animal_synset = wn.synset('animal.n.01')\r\n",
        "# getting sorted list of HYPONYMS from vehicle_synset\r\n",
        "print(animal_synset.substance_meronyms())\r\n",
        "print(animal_synset.part_meronyms())\r\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('animal_tissue.n.01')]\n",
            "[Synset('face.n.07'), Synset('head.n.01')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuJVZO3u0oEd"
      },
      "source": [
        "#### 4.Holonym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q2ARhgVTCI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9012b0-688b-43f8-c0f2-3c2636dabe73"
      },
      "source": [
        "human_synset = wn.synset('human.n.01')\r\n",
        "# hyponyms for human\r\n",
        "human_synset.hyponyms()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('homo_erectus.n.01'),\n",
              " Synset('homo_habilis.n.01'),\n",
              " Synset('homo_sapiens.n.01'),\n",
              " Synset('homo_soloensis.n.01'),\n",
              " Synset('neandertal_man.n.01'),\n",
              " Synset('rhodesian_man.n.01'),\n",
              " Synset('world.n.08')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rLTEuOA0oo7"
      },
      "source": [
        "#### 5.Entailment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MGQCvIeTCvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f1d70a-78e3-48ca-d14b-79a39697da57"
      },
      "source": [
        "stand_synset = wn.synset('stand.v.01')\r\n",
        "# entailment for verb stand\r\n",
        "stand_synset.entailments()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('arise.v.03')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    }
  ]
}