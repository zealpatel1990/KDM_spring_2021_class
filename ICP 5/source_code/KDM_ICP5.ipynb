{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM_ICP5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OUEr8ztPfwz",
        "outputId": "97b93d3b-d54c-4434-f235-b74b941b30a5"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=f8d393567a7c23f14f242db1330833f7f206108bf11f5e35970326f1d1dab072\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gFSCm5PPf96"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RqI5W9PwEU"
      },
      "source": [
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ynmbdljP37e"
      },
      "source": [
        "# creating spark dataframe wiht the input data. You can also read the data from file. label represents the 3 documnets (0.0,0.1,0.2)\r\n",
        "sentenceData = spark.createDataFrame([\r\n",
        "        (0.0, \"Welcome to KDM TF_IDF Tutorial.\"),\r\n",
        "        (0.1, \"Learn Spark ml tf_idf in today's lab.\"),\r\n",
        "        (0.2, \"Spark Mllib has TF-IDF.\")\r\n",
        "    ], [\"label\", \"sentence\"])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAuDw1JTP_j9"
      },
      "source": [
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(sentenceData)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juPnzf1HkBNi",
        "outputId": "dff68967-8f87-46e8-9dce-d881f0f35495"
      },
      "source": [
        "wordsData.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|\n",
            "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|\n",
            "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWWoLXIKQEp-"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\r\n",
        "featurizedData = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqsyc3TFQLX4"
      },
      "source": [
        "# calculating the IDF\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idfModel = idf.fit(featurizedData)\r\n",
        "rescaledData = idfModel.transform(featurizedData)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej95DLvjQS1T",
        "outputId": "70d9b478-0e5d-4a61-e662-cbd1a7e8c565"
      },
      "source": [
        "#displaying the results\r\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[2,8,13,15,17...|\n",
            "|  0.1|(20,[2,3,6,7],[0....|\n",
            "|  0.2|(20,[6,14,15],[0....|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFY0atcwQiCs"
      },
      "source": [
        "spark2 = SparkSession.builder.appName(\"Ngram Example\").getOrCreate()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j1F-kR6Q2Ay"
      },
      "source": [
        "#creating dataframe of input\r\n",
        "wordDataFrame = spark2.createDataFrame([\r\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\r\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\r\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\r\n",
        "], [\"id\", \"words\"])\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnz2_iOQ_xO"
      },
      "source": [
        "#creating NGrams with n=2 (two words)\r\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMhJK57bRGD5",
        "outputId": "68f54f73-9b14-4e26-ee2e-dafce9448da6"
      },
      "source": [
        "# displaying the results\r\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|ngrams                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Hi I, I heard, heard about, about Spark]                         |\n",
            "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|[Logistic regression, regression models, models are, are neat]    |\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzQ7KVW3Riz0"
      },
      "source": [
        "# creating spark session\r\n",
        "spark3 = SparkSession.builder.appName(\"Word2Vec Example\").getOrCreate()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzHNuEa5Ri9E"
      },
      "source": [
        "# Input data: Each row is a bag of words from a sentence or document.\r\n",
        "documentDF = spark3.createDataFrame([\r\n",
        "    (\"McCarthy was asked to analyse the data from the first phase of trials of the vaccine.\".split(\" \"), ),\r\n",
        "    (\"We have amassed the raw data and are about to begin analysing it.\".split(\" \"), ),\r\n",
        "    (\"Without more data we cannot make a meaningful comparison of the two systems.\".split(\" \"), ),\r\n",
        "    (\"Collecting data is a painfully slow process.\".split(\" \"), ),\r\n",
        "    (\"You need a long series of data to be able to discern such a trend.\".split(\" \"), )\r\n",
        "], [\"text\"])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IjCO6rRRjCu"
      },
      "source": [
        "# Learn a mapping from words to Vectors.\r\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\r\n",
        "model = word2Vec.fit(documentDF)\r\n",
        "result = model.transform(documentDF)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmVIfXsHSHUs",
        "outputId": "af3ca5a8-0977-4837-fce2-cdde2b1bec10"
      },
      "source": [
        "for row in result.collect():\r\n",
        "    text, vector = row\r\n",
        "    #printing the results\r\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: [McCarthy, was, asked, to, analyse, the, data, from, the, first, phase, of, trials, of, the, vaccine.] => \n",
            "Vector: [0.022913106746273115,0.007734941667877138,-0.002375059062615037]\n",
            "\n",
            "Text: [We, have, amassed, the, raw, data, and, are, about, to, begin, analysing, it.] => \n",
            "Vector: [-0.04099500154216702,0.0049125014159541866,-0.0014882660112701931]\n",
            "\n",
            "Text: [Without, more, data, we, cannot, make, a, meaningful, comparison, of, the, two, systems.] => \n",
            "Vector: [0.022649812046438456,0.0013891176248972234,0.01336516855427852]\n",
            "\n",
            "Text: [Collecting, data, is, a, painfully, slow, process.] => \n",
            "Vector: [-0.052166125604084554,0.04153742528121386,-0.018508063097085272]\n",
            "\n",
            "Text: [You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.] => \n",
            "Vector: [0.011940793506801129,0.020289071649312974,-0.0013307716076572736]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCbv_TB_SeqZ",
        "outputId": "daa0e433-a17e-4af7-ef6c-12bb76bb14bf"
      },
      "source": [
        "# showing the synonyms and cosine similarity of the word in input data\r\n",
        "synonyms = model.findSynonyms(\"data\", 5)   # its okay for certain words , real bad for others\r\n",
        "synonyms.show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------------+\n",
            "|      word|        similarity|\n",
            "+----------+------------------+\n",
            "|      long|0.9953629374504089|\n",
            "|    cannot|0.9902032613754272|\n",
            "|meaningful|0.9105675220489502|\n",
            "|      from|0.8469968438148499|\n",
            "|     asked| 0.794264554977417|\n",
            "+----------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdspYI3Qg1o"
      },
      "source": [
        "#closing the spark sessions\r\n",
        "spark.stop()\r\n",
        "spark2.stop()\r\n",
        "spark3.stop()"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}