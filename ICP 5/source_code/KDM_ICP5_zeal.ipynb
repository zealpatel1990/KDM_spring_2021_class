{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM_ICP5_zeal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zealpatel1990/KDM_spring_2021_class/blob/main/ICP%205/source_code/KDM_ICP5_zeal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OUEr8ztPfwz",
        "outputId": "a225f2fc-61ab-471e-fc11-2d61f982a63e"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 33kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 39.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=643e682b9677b01095c7d19cda29a053159faab6acf2372eb21f0525baf161b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gFSCm5PPf96"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RqI5W9PwEU"
      },
      "source": [
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ynmbdljP37e"
      },
      "source": [
        "# creating spark dataframe wiht the input data. You can also read the data from file. label represents the 3 documnets (0.0,0.1,0.2)\r\n",
        "sentenceData = spark.createDataFrame([\r\n",
        "        (0.0, \"Welcome to KDM TF_IDF Tutorial.\"),\r\n",
        "        (0.1, \"Learn Spark ml tf_idf in today's lab.\"),\r\n",
        "        (0.2, \"Spark Mllib has TF-IDF.\")\r\n",
        "    ], [\"label\", \"sentence\"])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAuDw1JTP_j9"
      },
      "source": [
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(sentenceData)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juPnzf1HkBNi",
        "outputId": "c76db941-0667-4b9f-dc72-8499ab67bbf7"
      },
      "source": [
        "wordsData.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|\n",
            "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|\n",
            "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWWoLXIKQEp-"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\r\n",
        "featurizedData = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqsyc3TFQLX4"
      },
      "source": [
        "# calculating the IDF\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idfModel = idf.fit(featurizedData)\r\n",
        "rescaledData = idfModel.transform(featurizedData)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej95DLvjQS1T",
        "outputId": "bf5bf4b1-1cc7-49a5-e305-48c867b010fd"
      },
      "source": [
        "#displaying the results\r\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[2,8,13,15,17...|\n",
            "|  0.1|(20,[2,3,6,7],[0....|\n",
            "|  0.2|(20,[6,14,15],[0....|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFY0atcwQiCs"
      },
      "source": [
        "spark2 = SparkSession.builder.appName(\"Ngram Example\").getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j1F-kR6Q2Ay"
      },
      "source": [
        "#creating dataframe of input\r\n",
        "wordDataFrame = spark2.createDataFrame([\r\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\r\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\r\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\r\n",
        "], [\"id\", \"words\"])\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnz2_iOQ_xO"
      },
      "source": [
        "#creating NGrams with n=2 (two words)\r\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMhJK57bRGD5",
        "outputId": "5f1355f4-6ed5-463e-cc36-f8fef8c3bb01"
      },
      "source": [
        "# displaying the results\r\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|ngrams                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Hi I, I heard, heard about, about Spark]                         |\n",
            "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|[Logistic regression, regression models, models are, are neat]    |\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzQ7KVW3Riz0"
      },
      "source": [
        "# creating spark session\r\n",
        "spark3 = SparkSession.builder.appName(\"Word2Vec Example\").getOrCreate()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzHNuEa5Ri9E"
      },
      "source": [
        "# Input data: Each row is a bag of words from a sentence or document.\r\n",
        "documentDF = spark3.createDataFrame([\r\n",
        "    (\"McCarthy was asked to analyse the data from the first phase of trials of the vaccine.\".split(\" \"), ),\r\n",
        "    (\"We have amassed the raw data and are about to begin analysing it.\".split(\" \"), ),\r\n",
        "    (\"Without more data we cannot make a meaningful comparison of the two systems.\".split(\" \"), ),\r\n",
        "    (\"Collecting data is a painfully slow process.\".split(\" \"), ),\r\n",
        "    (\"You need a long series of data to be able to discern such a trend.\".split(\" \"), )\r\n",
        "], [\"text\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IjCO6rRRjCu"
      },
      "source": [
        "# Learn a mapping from words to Vectors.\r\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\r\n",
        "model = word2Vec.fit(documentDF)\r\n",
        "result = model.transform(documentDF)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmVIfXsHSHUs",
        "outputId": "3d56e4c1-62b2-4f26-ed21-1809f3024fdc"
      },
      "source": [
        "for row in result.collect():\r\n",
        "    text, vector = row\r\n",
        "    #printing the results\r\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: [McCarthy, was, asked, to, analyse, the, data, from, the, first, phase, of, trials, of, the, vaccine.] => \n",
            "Vector: [0.008847468649037182,0.027308731980156153,0.005336133428500034]\n",
            "\n",
            "Text: [We, have, amassed, the, raw, data, and, are, about, to, begin, analysing, it.] => \n",
            "Vector: [0.01598473423375533,0.024079730084989794,0.022829564359898753]\n",
            "\n",
            "Text: [Without, more, data, we, cannot, make, a, meaningful, comparison, of, the, two, systems.] => \n",
            "Vector: [0.008413167097247565,-0.0008237379101606517,-0.04820680912010945]\n",
            "\n",
            "Text: [Collecting, data, is, a, painfully, slow, process.] => \n",
            "Vector: [-0.020028315750615935,0.03329797900680984,0.01771639713219234]\n",
            "\n",
            "Text: [You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.] => \n",
            "Vector: [0.007570979123314222,0.00809092471996943,0.003313163419564565]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCbv_TB_SeqZ",
        "outputId": "9ef755c3-35d5-476e-a142-ebf6a3dde49e"
      },
      "source": [
        "# showing the synonyms and cosine similarity of the word in input data\r\n",
        "synonyms = model.findSynonyms(\"data\", 5)   # its okay for certain words , real bad for others\r\n",
        "synonyms.show(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+------------------+\n",
            "| word|        similarity|\n",
            "+-----+------------------+\n",
            "|  raw|0.9751740097999573|\n",
            "|first|0.9728763699531555|\n",
            "|   We|0.8854662775993347|\n",
            "|   is|0.7463950514793396|\n",
            "|  it.|0.7444145679473877|\n",
            "+-----+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdspYI3Qg1o"
      },
      "source": [
        "#closing the spark sessions\r\n",
        "spark.stop()\r\n",
        "spark2.stop()\r\n",
        "spark3.stop()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoOaRb1FqcHF"
      },
      "source": [
        "# Task 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avE0cr83uP7X"
      },
      "source": [
        "with open(\"/content/t1.txt\",\"r+\") as t1:\r\n",
        "    doc1 = t1.read()\r\n",
        "with open(\"/content/t2.txt\",\"r+\") as t2:\r\n",
        "    doc2 = t2.read()\r\n",
        "with open(\"/content/t3.txt\",\"r+\") as t3:\r\n",
        "    doc3 = t3.read()\r\n",
        "with open(\"/content/t4.txt\",\"r+\") as t4:\r\n",
        "    doc4 = t4.read()\r\n",
        "with open(\"/content/t5.txt\",\"r+\") as t5:\r\n",
        "    doc5 = t5.read()\r\n",
        "# Read all 5 txt files which contains news articles\r\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk9bDpdzs0xo"
      },
      "source": [
        "## a. Top 10 TF-IDF words for the above input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-dO-GFouIpx",
        "outputId": "c14c3005-6d87-4754-a42d-4d0f90e182a4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "pd.set_option('display.max_columns', 20)\r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           said        on   bitcoin       for      that        in        to  \\\n",
            "0      0.044284  0.094525  0.000000  0.037253  0.088568  0.094525  0.157541   \n",
            "1      0.139950  0.033192  0.000000  0.078487  0.233249  0.066384  0.099575   \n",
            "2      0.124685  0.044357  0.000000  0.000000  0.000000  0.088714  0.044357   \n",
            "3      0.000000  0.078588  0.000000  0.139374  0.000000  0.196469  0.157175   \n",
            "4      0.000000  0.067475  0.354008  0.119665  0.094833  0.033737  0.168687   \n",
            "Total  0.308918  0.318136  0.354008  0.374779  0.416650  0.479829  0.627336   \n",
            "\n",
            "             of       and       the  \n",
            "0      0.157541  0.094525  0.535641  \n",
            "1      0.165959  0.232342  0.265534  \n",
            "2      0.177429  0.133071  0.443572  \n",
            "3      0.078588  0.196469  0.392938  \n",
            "4      0.202424  0.134950  0.236162  \n",
            "Total  0.781941  0.791357  1.873846  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FTiO7MBs5lP"
      },
      "source": [
        "## b. Top 10 TF-IDF words for the lemmatized input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "179LaIq-uJQx",
        "outputId": "e8718b3c-f283-4acc-dc17-3f155937fb6a"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "words1 = nltk.word_tokenize(doc1)\r\n",
        "words2 = nltk.word_tokenize(doc2)\r\n",
        "words3 = nltk.word_tokenize(doc3)\r\n",
        "words4 = nltk.word_tokenize(doc4)\r\n",
        "words5 = nltk.word_tokenize(doc5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "\r\n",
        "documents = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "           said        on   bitcoin       for      that        in        to  \\\n",
            "0      0.044284  0.094525  0.000000  0.037253  0.088568  0.094525  0.157541   \n",
            "1      0.139950  0.033192  0.000000  0.078487  0.233249  0.066384  0.099575   \n",
            "2      0.124685  0.044357  0.000000  0.000000  0.000000  0.088714  0.044357   \n",
            "3      0.000000  0.078588  0.000000  0.139374  0.000000  0.196469  0.157175   \n",
            "4      0.000000  0.067475  0.354008  0.119665  0.094833  0.033737  0.168687   \n",
            "Total  0.308918  0.318136  0.354008  0.374779  0.416650  0.479829  0.627336   \n",
            "\n",
            "             of       and       the  \n",
            "0      0.157541  0.094525  0.535641  \n",
            "1      0.165959  0.232342  0.265534  \n",
            "2      0.177429  0.133071  0.443572  \n",
            "3      0.078588  0.196469  0.392938  \n",
            "4      0.202424  0.134950  0.236162  \n",
            "Total  0.781941  0.791357  1.873846  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uk9qQjvs-U3"
      },
      "source": [
        "## c. Top 10 TF-IDF words for the n-gram based input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgik6OiDuJ9I",
        "outputId": "e1cbab70-454d-410f-f6ef-0f2531808d1d"
      },
      "source": [
        "# this function takes document and n int value to generate list of n grams\r\n",
        "def ngrams(input, n):\r\n",
        "    input = input.split(' ')\r\n",
        "    output = []\r\n",
        "    for i in range(len(input)-n+1):\r\n",
        "        output.append(input[i:i+n])\r\n",
        "    return output\r\n",
        "\r\n",
        "ngram_doc1 = ' '.join([' '.join(x) for x in ngrams(doc1, 3)])\r\n",
        "ngram_doc2 = ' '.join([' '.join(x) for x in ngrams(doc2, 3)])\r\n",
        "ngram_doc3 = ' '.join([' '.join(x) for x in ngrams(doc3, 3)])\r\n",
        "ngram_doc4 = ' '.join([' '.join(x) for x in ngrams(doc4, 3)])\r\n",
        "ngram_doc5 = ' '.join([' '.join(x) for x in ngrams(doc5, 3)])\r\n",
        "\r\n",
        "# documents = [ngram_doc1,ngram_doc2,ngram_doc3,ngram_doc4,ngram_doc5]\r\n",
        "\r\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]\r\n",
        "\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer( ngram_range=(3,3)) # TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "\r\n",
        "df.loc['Total'] = df.sum() # adding row to value total\r\n",
        "\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       judicial review proceedings  kingdom as named  \\\n",
            "0                         0.000000          0.000000   \n",
            "1                         0.000000          0.000000   \n",
            "2                         0.105409          0.105409   \n",
            "3                         0.000000          0.000000   \n",
            "4                         0.000000          0.000000   \n",
            "Total                     0.105409          0.105409   \n",
            "\n",
            "       joining other likeminded  as well as  by the end  the end of  \\\n",
            "0                      0.000000    0.000000     0.15861     0.15861   \n",
            "1                      0.000000    0.064461     0.00000     0.00000   \n",
            "2                      0.105409    0.000000     0.00000     0.00000   \n",
            "3                      0.000000    0.073451     0.00000     0.00000   \n",
            "4                      0.000000    0.000000     0.00000     0.00000   \n",
            "Total                  0.105409    0.137912     0.15861     0.15861   \n",
            "\n",
            "       the united states  republican accountability project  \\\n",
            "0                0.15861                           0.000000   \n",
            "1                0.00000                           0.159795   \n",
            "2                0.00000                           0.000000   \n",
            "3                0.00000                           0.000000   \n",
            "4                0.00000                           0.000000   \n",
            "Total            0.15861                           0.159795   \n",
            "\n",
            "       the republican accountability  the northern ireland  \n",
            "0                           0.000000              0.000000  \n",
            "1                           0.159795              0.000000  \n",
            "2                           0.000000              0.210819  \n",
            "3                           0.000000              0.000000  \n",
            "4                           0.000000              0.000000  \n",
            "Total                       0.159795              0.210819  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKRvhacqehQ"
      },
      "source": [
        "# Task 2:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udBRW5cmCsfr",
        "outputId": "78a84bbb-7f6d-46f1-d4c6-ee26bbbf5960"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, doc1),\r\n",
        "        (0.1, doc2),\r\n",
        "        (0.2, doc3),\r\n",
        "        (0.3, doc4),\r\n",
        "        (0.5, doc5)\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|In his first trip...|[in, his, first, ...|\n",
            "|  0.1|“It’s a really im...|[“it’s, a, really...|\n",
            "|  0.2|The protocol was ...|[the, protocol, w...|\n",
            "|  0.3|Copper prices cli...|[copper, prices, ...|\n",
            "|  0.5|Many of the same ...|[many, of, the, s...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0ETIBHXtDlH"
      },
      "source": [
        "## a. Try without NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKp-_YP-uACJ",
        "outputId": "323f5627-ee41-4d95-ebef-b13988d606d8"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF without NLP:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[1,3,5,7,9,1...|\n",
            "|  0.1|(200,[0,4,5,7,9,1...|\n",
            "|  0.2|(200,[7,8,17,20,2...|\n",
            "|  0.3|(200,[2,3,5,9,12,...|\n",
            "|  0.5|(200,[1,2,3,4,5,7...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF without NLP:\n",
            "Row(label=0.0, document='In his first trips as president, President Biden traveled to Wisconsin and Michigan to promote his vaccination rollout plan and the $1.9 trillion relief bill he hopes can restore the American economy.\\n\\nAfter an optimistic vow on Tuesday that any American who wanted a vaccine “could have one by the end of July this year,” Mr. Biden was asking for patience on Friday, saying the United States could be“approaching normalcy” by the end of the year. The week ended as winter storms hitting much of the country delayed the delivery of six million vaccines.\\n\\nMr. Biden addressed the virtual G7 summit and said that his administration would make good on a U.S. promise to donate $4 billion to the global vaccination campaign over the next two years. Mr. Biden’s engagement in the global fight against the pandemic is in stark contrast to the approach of former President Donald J. Trump, who withdrew the United States from the World Health Organization.', words=['in', 'his', 'first', 'trips', 'as', 'president,', 'president', 'biden', 'traveled', 'to', 'wisconsin', 'and', 'michigan', 'to', 'promote', 'his', 'vaccination', 'rollout', 'plan', 'and', 'the', '$1.9', 'trillion', 'relief', 'bill', 'he', 'hopes', 'can', 'restore', 'the', 'american', 'economy.', '', 'after', 'an', 'optimistic', 'vow', 'on', 'tuesday', 'that', 'any', 'american', 'who', 'wanted', 'a', 'vaccine', '“could', 'have', 'one', 'by', 'the', 'end', 'of', 'july', 'this', 'year,”', 'mr.', 'biden', 'was', 'asking', 'for', 'patience', 'on', 'friday,', 'saying', 'the', 'united', 'states', 'could', 'be“approaching', 'normalcy”', 'by', 'the', 'end', 'of', 'the', 'year.', 'the', 'week', 'ended', 'as', 'winter', 'storms', 'hitting', 'much', 'of', 'the', 'country', 'delayed', 'the', 'delivery', 'of', 'six', 'million', 'vaccines.', '', 'mr.', 'biden', 'addressed', 'the', 'virtual', 'g7', 'summit', 'and', 'said', 'that', 'his', 'administration', 'would', 'make', 'good', 'on', 'a', 'u.s.', 'promise', 'to', 'donate', '$4', 'billion', 'to', 'the', 'global', 'vaccination', 'campaign', 'over', 'the', 'next', 'two', 'years.', 'mr.', 'biden’s', 'engagement', 'in', 'the', 'global', 'fight', 'against', 'the', 'pandemic', 'is', 'in', 'stark', 'contrast', 'to', 'the', 'approach', 'of', 'former', 'president', 'donald', 'j.', 'trump,', 'who', 'withdrew', 'the', 'united', 'states', 'from', 'the', 'world', 'health', 'organization.'], rawFeatures=SparseVector(200, {1: 1.0, 3: 1.0, 5: 3.0, 7: 1.0, 9: 1.0, 15: 1.0, 16: 2.0, 17: 17.0, 18: 1.0, 19: 1.0, 22: 2.0, 23: 2.0, 26: 1.0, 39: 1.0, 40: 3.0, 41: 1.0, 43: 1.0, 46: 1.0, 50: 2.0, 51: 1.0, 52: 2.0, 53: 2.0, 55: 1.0, 58: 1.0, 63: 3.0, 67: 4.0, 69: 1.0, 70: 2.0, 71: 3.0, 73: 2.0, 75: 2.0, 79: 1.0, 80: 2.0, 81: 1.0, 83: 2.0, 88: 5.0, 90: 2.0, 91: 3.0, 95: 6.0, 99: 1.0, 104: 1.0, 106: 2.0, 111: 1.0, 116: 1.0, 117: 3.0, 119: 1.0, 120: 2.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 2.0, 131: 1.0, 132: 2.0, 133: 1.0, 135: 4.0, 136: 3.0, 144: 2.0, 146: 1.0, 148: 1.0, 149: 2.0, 151: 1.0, 152: 2.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 159: 1.0, 160: 5.0, 163: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 172: 2.0, 173: 1.0, 174: 1.0, 175: 2.0, 180: 1.0, 181: 3.0, 182: 2.0, 184: 1.0, 185: 1.0, 189: 1.0, 195: 1.0}), features=SparseVector(200, {1: 0.6931, 3: 0.4055, 5: 0.547, 7: 0.1823, 9: 0.1823, 15: 0.6931, 16: 1.3863, 17: 0.0, 18: 0.6931, 19: 1.0986, 22: 1.3863, 23: 0.3646, 26: 0.4055, 39: 1.0986, 40: 2.0794, 41: 0.4055, 43: 0.6931, 46: 0.6931, 50: 0.3646, 51: 0.1823, 52: 2.1972, 53: 1.3863, 55: 0.1823, 58: 0.4055, 63: 0.0, 67: 0.0, 69: 0.6931, 70: 1.3863, 71: 3.2958, 73: 0.8109, 75: 0.8109, 79: 1.0986, 80: 1.3863, 81: 0.6931, 83: 0.8109, 88: 0.0, 90: 1.3863, 91: 0.0, 95: 0.0, 99: 0.1823, 104: 0.4055, 106: 1.3863, 111: 0.4055, 116: 1.0986, 117: 2.0794, 119: 0.4055, 120: 0.3646, 121: 0.0, 122: 0.1823, 123: 0.6931, 124: 0.4055, 125: 0.6931, 126: 0.4055, 127: 1.0986, 128: 1.3863, 131: 1.0986, 132: 1.3863, 133: 0.0, 135: 2.7726, 136: 0.0, 144: 0.3646, 146: 0.4055, 148: 0.1823, 149: 0.0, 151: 0.1823, 152: 0.8109, 153: 0.4055, 154: 0.6931, 155: 0.4055, 156: 1.0986, 157: 0.4055, 159: 0.6931, 160: 2.0273, 163: 0.1823, 166: 0.6931, 167: 0.1823, 168: 0.6931, 169: 0.4055, 172: 0.0, 173: 0.1823, 174: 0.6931, 175: 1.3863, 180: 1.0986, 181: 0.547, 182: 2.1972, 184: 0.4055, 185: 0.6931, 189: 0.4055, 195: 0.6931}))\n",
            "(200,[1,3,5,7,9,15,16,17,18,19,22,23,26,39,40,41,43,46,50,51,52,53,55,58,63,67,69,70,71,73,75,79,80,81,83,88,90,91,95,99,104,106,111,116,117,119,120,121,122,123,124,125,126,127,128,131,132,133,135,136,144,146,148,149,151,152,153,154,155,156,157,159,160,163,166,167,168,169,172,173,174,175,180,181,182,184,185,189,195],[1.0,1.0,3.0,1.0,1.0,1.0,2.0,17.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,3.0,4.0,1.0,2.0,3.0,2.0,2.0,1.0,2.0,1.0,2.0,5.0,2.0,3.0,6.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,4.0,3.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document=\"“It’s a really important thing to do, to thank these Republicans who really took a stand against Trump and took chances that obviously have been challenging and will continue to be challenging,” said Olivia Troye, co-director of the Republican Accountability Project. “You've seen them be censured by their own parties in their own states and districts, and the dynamic that’s come and the hatred and fury that you see from some of the supporters against them.”\\n\\nTroye said that the organization is targeting voters on Fox News because “there’s a whole voting population of Republican voters right now that are not OK with what happened” as well as Trump voters for whom the Jan. 6 insurrection “really gave them pause.”\\n\\nThe Republican Accountability Project said it will defend all of the House and Senate members who impeached or convicted Trump as part of its campaign, including those who are retiring or not up for reelection in 2022.\", words=['“it’s', 'a', 'really', 'important', 'thing', 'to', 'do,', 'to', 'thank', 'these', 'republicans', 'who', 'really', 'took', 'a', 'stand', 'against', 'trump', 'and', 'took', 'chances', 'that', 'obviously', 'have', 'been', 'challenging', 'and', 'will', 'continue', 'to', 'be', 'challenging,”', 'said', 'olivia', 'troye,', 'co-director', 'of', 'the', 'republican', 'accountability', 'project.', \"“you've\", 'seen', 'them', 'be', 'censured', 'by', 'their', 'own', 'parties', 'in', 'their', 'own', 'states', 'and', 'districts,', 'and', 'the', 'dynamic', 'that’s', 'come', 'and', 'the', 'hatred', 'and', 'fury', 'that', 'you', 'see', 'from', 'some', 'of', 'the', 'supporters', 'against', 'them.”', '', 'troye', 'said', 'that', 'the', 'organization', 'is', 'targeting', 'voters', 'on', 'fox', 'news', 'because', '“there’s', 'a', 'whole', 'voting', 'population', 'of', 'republican', 'voters', 'right', 'now', 'that', 'are', 'not', 'ok', 'with', 'what', 'happened”', 'as', 'well', 'as', 'trump', 'voters', 'for', 'whom', 'the', 'jan.', '6', 'insurrection', '“really', 'gave', 'them', 'pause.”', '', 'the', 'republican', 'accountability', 'project', 'said', 'it', 'will', 'defend', 'all', 'of', 'the', 'house', 'and', 'senate', 'members', 'who', 'impeached', 'or', 'convicted', 'trump', 'as', 'part', 'of', 'its', 'campaign,', 'including', 'those', 'who', 'are', 'retiring', 'or', 'not', 'up', 'for', 'reelection', 'in', '2022.'], rawFeatures=SparseVector(200, {0: 1.0, 4: 1.0, 5: 5.0, 7: 1.0, 9: 2.0, 10: 1.0, 16: 4.0, 17: 9.0, 20: 3.0, 23: 1.0, 25: 1.0, 27: 1.0, 30: 2.0, 31: 1.0, 33: 1.0, 34: 1.0, 35: 2.0, 36: 1.0, 41: 1.0, 42: 2.0, 44: 1.0, 45: 1.0, 49: 1.0, 50: 1.0, 53: 1.0, 55: 2.0, 57: 2.0, 58: 1.0, 63: 3.0, 66: 1.0, 67: 4.0, 72: 1.0, 73: 1.0, 76: 1.0, 77: 1.0, 81: 1.0, 84: 1.0, 86: 4.0, 87: 1.0, 88: 3.0, 89: 1.0, 91: 8.0, 95: 5.0, 99: 1.0, 100: 1.0, 101: 1.0, 103: 2.0, 111: 1.0, 112: 5.0, 114: 2.0, 120: 4.0, 121: 1.0, 124: 2.0, 126: 1.0, 129: 1.0, 132: 1.0, 133: 2.0, 134: 4.0, 135: 1.0, 136: 2.0, 138: 1.0, 140: 1.0, 141: 2.0, 143: 1.0, 144: 2.0, 147: 2.0, 149: 3.0, 151: 1.0, 154: 1.0, 157: 1.0, 160: 4.0, 163: 3.0, 164: 2.0, 167: 3.0, 168: 1.0, 169: 2.0, 172: 4.0, 173: 1.0, 181: 1.0, 185: 1.0, 190: 1.0, 193: 1.0, 198: 1.0}), features=SparseVector(200, {0: 1.0986, 4: 0.6931, 5: 0.9116, 7: 0.1823, 9: 0.3646, 10: 1.0986, 16: 2.7726, 17: 0.0, 20: 0.547, 23: 0.1823, 25: 1.0986, 27: 1.0986, 30: 0.8109, 31: 1.0986, 33: 0.6931, 34: 0.6931, 35: 1.3863, 36: 1.0986, 41: 0.4055, 42: 1.3863, 44: 0.6931, 45: 0.6931, 49: 1.0986, 50: 0.1823, 53: 0.6931, 55: 0.3646, 57: 0.8109, 58: 0.4055, 63: 0.0, 66: 0.1823, 67: 0.0, 72: 0.6931, 73: 0.4055, 76: 0.4055, 77: 0.6931, 81: 0.6931, 84: 1.0986, 86: 2.7726, 87: 0.6931, 88: 0.0, 89: 0.4055, 91: 0.0, 95: 0.0, 99: 0.1823, 100: 0.6931, 101: 0.6931, 103: 1.3863, 111: 0.4055, 112: 3.4657, 114: 0.8109, 120: 0.7293, 121: 0.0, 124: 0.8109, 126: 0.4055, 129: 1.0986, 132: 0.6931, 133: 0.0, 134: 1.6219, 135: 0.6931, 136: 0.0, 138: 0.4055, 140: 0.6931, 141: 2.1972, 143: 0.6931, 144: 0.3646, 147: 2.1972, 149: 0.0, 151: 0.1823, 154: 0.6931, 157: 0.4055, 160: 1.6219, 163: 0.547, 164: 0.8109, 167: 0.547, 168: 0.6931, 169: 0.8109, 172: 0.0, 173: 0.1823, 181: 0.1823, 185: 0.6931, 190: 1.0986, 193: 0.6931, 198: 0.6931}))\n",
            "(200,[0,4,5,7,9,10,16,17,20,23,25,27,30,31,33,34,35,36,41,42,44,45,49,50,53,55,57,58,63,66,67,72,73,76,77,81,84,86,87,88,89,91,95,99,100,101,103,111,112,114,120,121,124,126,129,132,133,134,135,136,138,140,141,143,144,147,149,151,154,157,160,163,164,167,168,169,172,173,181,185,190,193,198],[1.0,1.0,5.0,1.0,2.0,1.0,4.0,9.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,3.0,1.0,8.0,5.0,1.0,1.0,1.0,2.0,1.0,5.0,2.0,4.0,1.0,2.0,1.0,1.0,1.0,2.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,4.0,3.0,2.0,3.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.2, document='The protocol was designed to protect the European Union\\'s single market without creating a land border on the island of Ireland.\\n\\nLeader Arlene Foster, Deputy Leader Nigel Dodds and members of parliament Jeffrey Donaldson and Sammy Wilson will join in the action challenging the protocol, a DUP statement said.\\n\\nThey will be \"joining other likeminded unionists from across the United Kingdom as named parties in judicial review proceedings challenging the Northern Ireland Protocol’s compatibility with Act of Union 1800, the Northern Ireland Act of 1998 and the Belfast Agreement,\" the statement said.', words=['the', 'protocol', 'was', 'designed', 'to', 'protect', 'the', 'european', \"union's\", 'single', 'market', 'without', 'creating', 'a', 'land', 'border', 'on', 'the', 'island', 'of', 'ireland.', '', 'leader', 'arlene', 'foster,', 'deputy', 'leader', 'nigel', 'dodds', 'and', 'members', 'of', 'parliament', 'jeffrey', 'donaldson', 'and', 'sammy', 'wilson', 'will', 'join', 'in', 'the', 'action', 'challenging', 'the', 'protocol,', 'a', 'dup', 'statement', 'said.', '', 'they', 'will', 'be', '\"joining', 'other', 'likeminded', 'unionists', 'from', 'across', 'the', 'united', 'kingdom', 'as', 'named', 'parties', 'in', 'judicial', 'review', 'proceedings', 'challenging', 'the', 'northern', 'ireland', 'protocol’s', 'compatibility', 'with', 'act', 'of', 'union', '1800,', 'the', 'northern', 'ireland', 'act', 'of', '1998', 'and', 'the', 'belfast', 'agreement,\"', 'the', 'statement', 'said.'], rawFeatures=SparseVector(200, {7: 1.0, 8: 1.0, 17: 12.0, 20: 2.0, 23: 1.0, 24: 2.0, 33: 1.0, 35: 1.0, 37: 1.0, 43: 2.0, 44: 1.0, 48: 1.0, 50: 1.0, 51: 1.0, 55: 1.0, 57: 1.0, 60: 1.0, 63: 2.0, 66: 1.0, 67: 2.0, 74: 1.0, 75: 1.0, 78: 1.0, 80: 1.0, 83: 1.0, 88: 1.0, 89: 2.0, 91: 3.0, 93: 1.0, 95: 4.0, 98: 2.0, 104: 1.0, 107: 2.0, 108: 1.0, 111: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 126: 2.0, 128: 2.0, 133: 3.0, 134: 2.0, 136: 2.0, 140: 1.0, 145: 1.0, 146: 2.0, 148: 1.0, 149: 4.0, 152: 1.0, 163: 1.0, 164: 1.0, 167: 1.0, 170: 1.0, 171: 2.0, 172: 2.0, 189: 2.0, 195: 1.0}), features=SparseVector(200, {7: 0.1823, 8: 1.0986, 17: 0.0, 20: 0.3646, 23: 0.1823, 24: 0.8109, 33: 0.6931, 35: 0.6931, 37: 0.4055, 43: 1.3863, 44: 0.6931, 48: 0.6931, 50: 0.1823, 51: 0.1823, 55: 0.1823, 57: 0.4055, 60: 0.4055, 63: 0.0, 66: 0.1823, 67: 0.0, 74: 1.0986, 75: 0.4055, 78: 1.0986, 80: 0.6931, 83: 0.4055, 88: 0.0, 89: 0.8109, 91: 0.0, 93: 0.6931, 95: 0.0, 98: 1.3863, 104: 0.4055, 107: 2.1972, 108: 1.0986, 111: 0.4055, 120: 0.1823, 121: 0.0, 122: 0.1823, 126: 0.8109, 128: 1.3863, 133: 0.0, 134: 0.8109, 136: 0.0, 140: 0.6931, 145: 1.0986, 146: 0.8109, 148: 0.1823, 149: 0.0, 152: 0.4055, 163: 0.1823, 164: 0.4055, 167: 0.1823, 170: 0.6931, 171: 1.3863, 172: 0.0, 189: 0.8109, 195: 0.6931}))\n",
            "(200,[7,8,17,20,23,24,33,35,37,43,44,48,50,51,55,57,60,63,66,67,74,75,78,80,83,88,89,91,93,95,98,104,107,108,111,120,121,122,126,128,133,134,136,140,145,146,148,149,152,163,164,167,170,171,172,189,195],[1.0,1.0,12.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,4.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,3.0,2.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0])\n",
            "Row(label=0.3, document=\"Copper prices climbed to their highest level in nearly a decade Friday as investors in the commodity anticipate rising demand for infrastructure and construction projects in the post-pandemic economy.\\n\\nOn Friday, the price of copper climbed above $4 per pound for the first time since September 2011.\\nThe base metal is used in many construction materials, including electrical wires and water pipes. The uptick is being driven by short and long term optimism for the commodity due to both strong expected demand and supply constraints.\\nWith the full reopening of the US economy on the horizon and President Joe Biden's plan to invest heavily in America's infrastructure, as well as China's ongoing economic recovery, there is reason to believe copper demand will remain high.\", words=['copper', 'prices', 'climbed', 'to', 'their', 'highest', 'level', 'in', 'nearly', 'a', 'decade', 'friday', 'as', 'investors', 'in', 'the', 'commodity', 'anticipate', 'rising', 'demand', 'for', 'infrastructure', 'and', 'construction', 'projects', 'in', 'the', 'post-pandemic', 'economy.', '', 'on', 'friday,', 'the', 'price', 'of', 'copper', 'climbed', 'above', '$4', 'per', 'pound', 'for', 'the', 'first', 'time', 'since', 'september', '2011.', 'the', 'base', 'metal', 'is', 'used', 'in', 'many', 'construction', 'materials,', 'including', 'electrical', 'wires', 'and', 'water', 'pipes.', 'the', 'uptick', 'is', 'being', 'driven', 'by', 'short', 'and', 'long', 'term', 'optimism', 'for', 'the', 'commodity', 'due', 'to', 'both', 'strong', 'expected', 'demand', 'and', 'supply', 'constraints.', 'with', 'the', 'full', 'reopening', 'of', 'the', 'us', 'economy', 'on', 'the', 'horizon', 'and', 'president', 'joe', \"biden's\", 'plan', 'to', 'invest', 'heavily', 'in', \"america's\", 'infrastructure,', 'as', 'well', 'as', \"china's\", 'ongoing', 'economic', 'recovery,', 'there', 'is', 'reason', 'to', 'believe', 'copper', 'demand', 'will', 'remain', 'high.'], rawFeatures=SparseVector(200, {2: 1.0, 3: 1.0, 5: 2.0, 9: 3.0, 12: 1.0, 17: 11.0, 20: 2.0, 24: 1.0, 26: 1.0, 30: 1.0, 32: 1.0, 37: 3.0, 46: 1.0, 47: 1.0, 50: 1.0, 51: 1.0, 56: 1.0, 60: 1.0, 62: 1.0, 63: 6.0, 66: 1.0, 67: 1.0, 69: 1.0, 70: 1.0, 72: 3.0, 73: 1.0, 76: 1.0, 77: 1.0, 83: 1.0, 87: 4.0, 88: 4.0, 91: 5.0, 94: 1.0, 95: 2.0, 97: 3.0, 98: 1.0, 99: 1.0, 104: 3.0, 106: 1.0, 114: 1.0, 119: 2.0, 121: 1.0, 122: 2.0, 123: 2.0, 133: 1.0, 134: 1.0, 136: 2.0, 138: 1.0, 144: 5.0, 148: 1.0, 149: 4.0, 151: 1.0, 153: 2.0, 155: 2.0, 157: 4.0, 164: 1.0, 166: 1.0, 167: 1.0, 171: 1.0, 172: 1.0, 173: 1.0, 176: 1.0, 181: 1.0, 184: 1.0, 186: 1.0, 188: 3.0, 189: 1.0, 192: 1.0, 197: 1.0}), features=SparseVector(200, {2: 0.6931, 3: 0.4055, 5: 0.3646, 9: 0.547, 12: 0.6931, 17: 0.0, 20: 0.3646, 24: 0.4055, 26: 0.4055, 30: 0.4055, 32: 1.0986, 37: 1.2164, 46: 0.6931, 47: 1.0986, 50: 0.1823, 51: 0.1823, 56: 0.6931, 60: 0.4055, 62: 1.0986, 63: 0.0, 66: 0.1823, 67: 0.0, 69: 0.6931, 70: 0.6931, 72: 2.0794, 73: 0.4055, 76: 0.4055, 77: 0.6931, 83: 0.4055, 87: 2.7726, 88: 0.0, 91: 0.0, 94: 1.0986, 95: 0.0, 97: 2.0794, 98: 0.6931, 99: 0.1823, 104: 1.2164, 106: 0.6931, 114: 0.4055, 119: 0.8109, 121: 0.0, 122: 0.3646, 123: 1.3863, 133: 0.0, 134: 0.4055, 136: 0.0, 138: 0.4055, 144: 0.9116, 148: 0.1823, 149: 0.0, 151: 0.1823, 153: 0.8109, 155: 0.8109, 157: 1.6219, 164: 0.4055, 166: 0.6931, 167: 0.1823, 171: 0.6931, 172: 0.0, 173: 0.1823, 176: 1.0986, 181: 0.1823, 184: 0.4055, 186: 1.0986, 188: 2.0794, 189: 0.4055, 192: 0.6931, 197: 1.0986}))\n",
            "(200,[2,3,5,9,12,17,20,24,26,30,32,37,46,47,50,51,56,60,62,63,66,67,69,70,72,73,76,77,83,87,88,91,94,95,97,98,99,104,106,114,119,121,122,123,133,134,136,138,144,148,149,151,153,155,157,164,166,167,171,172,173,176,181,184,186,188,189,192,197],[1.0,1.0,2.0,3.0,1.0,11.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,4.0,4.0,5.0,1.0,2.0,3.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,5.0,1.0,4.0,1.0,2.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document=\"Many of the same theses that bulls have extolled for years continue to drive Bitcoin's ascent. For example, fans still view its 21 million token limit as a hedge against the long-term devaluation of the U.S. dollar as the money supply increases.\\n\\nEnthusiasts also still value Bitcoin for its payment potential. A greater number of businesses are accepting cryptocurrency now than ever before, and Bitcoin's network can settle payment transactions in an average of 10 minutes. This compares to cross-border payments on traditional networks, which can take up to a week to be validated and settled.\\n\\nThe world's most popular digital currency has also received a boost from brand-name enterprise adoption. Electric-vehicle maker Tesla recently announced a $1.5 billion purchase of Bitcoin, which will be added to the automaker's balance sheet.\\n\\nBut you might be disappointed if you dig beyond the short-term cheerleading and technical analysis. History is clear that when next-big-thing investments go parabolic, they eventually implode. Furthermore, Bitcoin is full of flaws and misconceptions perpetuated on social media platforms.\\n\", words=['many', 'of', 'the', 'same', 'theses', 'that', 'bulls', 'have', 'extolled', 'for', 'years', 'continue', 'to', 'drive', \"bitcoin's\", 'ascent.', 'for', 'example,', 'fans', 'still', 'view', 'its', '21', 'million', 'token', 'limit', 'as', 'a', 'hedge', 'against', 'the', 'long-term', 'devaluation', 'of', 'the', 'u.s.', 'dollar', 'as', 'the', 'money', 'supply', 'increases.', '', 'enthusiasts', 'also', 'still', 'value', 'bitcoin', 'for', 'its', 'payment', 'potential.', 'a', 'greater', 'number', 'of', 'businesses', 'are', 'accepting', 'cryptocurrency', 'now', 'than', 'ever', 'before,', 'and', \"bitcoin's\", 'network', 'can', 'settle', 'payment', 'transactions', 'in', 'an', 'average', 'of', '10', 'minutes.', 'this', 'compares', 'to', 'cross-border', 'payments', 'on', 'traditional', 'networks,', 'which', 'can', 'take', 'up', 'to', 'a', 'week', 'to', 'be', 'validated', 'and', 'settled.', '', 'the', \"world's\", 'most', 'popular', 'digital', 'currency', 'has', 'also', 'received', 'a', 'boost', 'from', 'brand-name', 'enterprise', 'adoption.', 'electric-vehicle', 'maker', 'tesla', 'recently', 'announced', 'a', '$1.5', 'billion', 'purchase', 'of', 'bitcoin,', 'which', 'will', 'be', 'added', 'to', 'the', \"automaker's\", 'balance', 'sheet.', '', 'but', 'you', 'might', 'be', 'disappointed', 'if', 'you', 'dig', 'beyond', 'the', 'short-term', 'cheerleading', 'and', 'technical', 'analysis.', 'history', 'is', 'clear', 'that', 'when', 'next-big-thing', 'investments', 'go', 'parabolic,', 'they', 'eventually', 'implode.', 'furthermore,', 'bitcoin', 'is', 'full', 'of', 'flaws', 'and', 'misconceptions', 'perpetuated', 'on', 'social', 'media', 'platforms.'], rawFeatures=SparseVector(200, {1: 2.0, 2: 1.0, 3: 1.0, 4: 2.0, 5: 1.0, 7: 2.0, 9: 3.0, 11: 1.0, 12: 1.0, 13: 1.0, 15: 1.0, 17: 7.0, 18: 2.0, 20: 1.0, 21: 2.0, 22: 1.0, 23: 1.0, 24: 4.0, 26: 1.0, 30: 1.0, 34: 2.0, 37: 1.0, 40: 2.0, 41: 1.0, 42: 1.0, 45: 1.0, 48: 1.0, 51: 2.0, 54: 1.0, 55: 4.0, 56: 1.0, 57: 1.0, 58: 2.0, 60: 1.0, 61: 1.0, 63: 1.0, 64: 2.0, 66: 1.0, 67: 7.0, 75: 1.0, 76: 1.0, 85: 1.0, 86: 1.0, 88: 6.0, 89: 1.0, 90: 1.0, 91: 5.0, 92: 2.0, 93: 1.0, 95: 6.0, 97: 1.0, 99: 1.0, 100: 3.0, 101: 2.0, 102: 1.0, 103: 1.0, 112: 3.0, 114: 2.0, 117: 1.0, 119: 1.0, 120: 1.0, 121: 3.0, 122: 1.0, 124: 1.0, 125: 3.0, 133: 2.0, 136: 3.0, 138: 3.0, 143: 2.0, 144: 4.0, 146: 1.0, 148: 1.0, 149: 2.0, 151: 2.0, 152: 1.0, 153: 1.0, 155: 2.0, 159: 1.0, 160: 2.0, 161: 3.0, 163: 1.0, 165: 1.0, 169: 3.0, 170: 1.0, 172: 3.0, 173: 3.0, 174: 1.0, 175: 1.0, 179: 1.0, 181: 1.0, 184: 1.0, 188: 1.0, 192: 2.0, 193: 1.0, 194: 1.0, 198: 1.0, 199: 1.0}), features=SparseVector(200, {1: 1.3863, 2: 0.6931, 3: 0.4055, 4: 1.3863, 5: 0.1823, 7: 0.3646, 9: 0.547, 11: 1.0986, 12: 0.6931, 13: 1.0986, 15: 0.6931, 17: 0.0, 18: 1.3863, 20: 0.1823, 21: 2.1972, 22: 0.6931, 23: 0.1823, 24: 1.6219, 26: 0.4055, 30: 0.4055, 34: 1.3863, 37: 0.4055, 40: 1.3863, 41: 0.4055, 42: 0.6931, 45: 0.6931, 48: 0.6931, 51: 0.3646, 54: 1.0986, 55: 0.7293, 56: 0.6931, 57: 0.4055, 58: 0.8109, 60: 0.4055, 61: 1.0986, 63: 0.0, 64: 2.1972, 66: 0.1823, 67: 0.0, 75: 0.4055, 76: 0.4055, 85: 1.0986, 86: 0.6931, 88: 0.0, 89: 0.4055, 90: 0.6931, 91: 0.0, 92: 2.1972, 93: 0.6931, 95: 0.0, 97: 0.6931, 99: 0.1823, 100: 2.0794, 101: 1.3863, 102: 1.0986, 103: 0.6931, 112: 2.0794, 114: 0.8109, 117: 0.6931, 119: 0.4055, 120: 0.1823, 121: 0.0, 122: 0.1823, 124: 0.4055, 125: 2.0794, 133: 0.0, 136: 0.0, 138: 1.2164, 143: 1.3863, 144: 0.7293, 146: 0.4055, 148: 0.1823, 149: 0.0, 151: 0.3646, 152: 0.4055, 153: 0.4055, 155: 0.8109, 159: 0.6931, 160: 0.8109, 161: 3.2958, 163: 0.1823, 165: 1.0986, 169: 1.2164, 170: 0.6931, 172: 0.0, 173: 0.547, 174: 0.6931, 175: 0.6931, 179: 1.0986, 181: 0.1823, 184: 0.4055, 188: 0.6931, 192: 1.3863, 193: 0.6931, 194: 1.0986, 198: 0.6931, 199: 1.0986}))\n",
            "(200,[1,2,3,4,5,7,9,11,12,13,15,17,18,20,21,22,23,24,26,30,34,37,40,41,42,45,48,51,54,55,56,57,58,60,61,63,64,66,67,75,76,85,86,88,89,90,91,92,93,95,97,99,100,101,102,103,112,114,117,119,120,121,122,124,125,133,136,138,143,144,146,148,149,151,152,153,155,159,160,161,163,165,169,170,172,173,174,175,179,181,184,188,192,193,194,198,199],[2.0,1.0,1.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,7.0,2.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,7.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,5.0,2.0,1.0,6.0,1.0,1.0,3.0,2.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,2.0,3.0,3.0,2.0,4.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,3.0,1.0,1.0,3.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTdb4zeet3TP"
      },
      "source": [
        "## b. Try with Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIembjTeREHk",
        "outputId": "3511d1c8-b068-47a4-e1f8-fec1da0bd64d"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "words1 = nltk.word_tokenize(doc1)\r\n",
        "words2 = nltk.word_tokenize(doc2)\r\n",
        "words3 = nltk.word_tokenize(doc3)\r\n",
        "words4 = nltk.word_tokenize(doc4)\r\n",
        "words5 = nltk.word_tokenize(doc5)\r\n",
        "\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "\r\n",
        "### lemmatizing words from 5 input docs same as previos task\r\n",
        "\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, lemmatized_document1),\r\n",
        "        (0.1, lemmatized_document2),\r\n",
        "        (0.2, lemmatized_document3),\r\n",
        "        (0.3, lemmatized_document4),\r\n",
        "        (0.5, lemmatized_document5)\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|In his first trip...|[in, his, first, ...|\n",
            "|  0.1|“ It ’ s a really...|[“, it, ’, s, a, ...|\n",
            "|  0.2|The protocol wa d...|[the, protocol, w...|\n",
            "|  0.3|Copper price clim...|[copper, price, c...|\n",
            "|  0.5|Many of the same ...|[many, of, the, s...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0PSkDOWRsPQ",
        "outputId": "9a332d80-7be6-46b3-ff57-af7ca81efe37"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with Lemmatization:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[0,1,3,5,9,1...|\n",
            "|  0.1|(200,[0,4,5,8,9,1...|\n",
            "|  0.2|(200,[7,8,13,17,2...|\n",
            "|  0.3|(200,[3,4,5,9,17,...|\n",
            "|  0.5|(200,[1,2,3,4,5,6...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF without NLP:\n",
            "Row(label=0.0, document='In his first trip a president , President Biden traveled to Wisconsin and Michigan to promote his vaccination rollout plan and the $ 1.9 trillion relief bill he hope can restore the American economy . After an optimistic vow on Tuesday that any American who wanted a vaccine “ could have one by the end of July this year , ” Mr. Biden wa asking for patience on Friday , saying the United States could be “ approaching normalcy ” by the end of the year . The week ended a winter storm hitting much of the country delayed the delivery of six million vaccine . Mr. Biden addressed the virtual G7 summit and said that his administration would make good on a U.S. promise to donate $ 4 billion to the global vaccination campaign over the next two year . Mr. Biden ’ s engagement in the global fight against the pandemic is in stark contrast to the approach of former President Donald J. Trump , who withdrew the United States from the World Health Organization .', words=['in', 'his', 'first', 'trip', 'a', 'president', ',', 'president', 'biden', 'traveled', 'to', 'wisconsin', 'and', 'michigan', 'to', 'promote', 'his', 'vaccination', 'rollout', 'plan', 'and', 'the', '$', '1.9', 'trillion', 'relief', 'bill', 'he', 'hope', 'can', 'restore', 'the', 'american', 'economy', '.', 'after', 'an', 'optimistic', 'vow', 'on', 'tuesday', 'that', 'any', 'american', 'who', 'wanted', 'a', 'vaccine', '“', 'could', 'have', 'one', 'by', 'the', 'end', 'of', 'july', 'this', 'year', ',', '”', 'mr.', 'biden', 'wa', 'asking', 'for', 'patience', 'on', 'friday', ',', 'saying', 'the', 'united', 'states', 'could', 'be', '“', 'approaching', 'normalcy', '”', 'by', 'the', 'end', 'of', 'the', 'year', '.', 'the', 'week', 'ended', 'a', 'winter', 'storm', 'hitting', 'much', 'of', 'the', 'country', 'delayed', 'the', 'delivery', 'of', 'six', 'million', 'vaccine', '.', 'mr.', 'biden', 'addressed', 'the', 'virtual', 'g7', 'summit', 'and', 'said', 'that', 'his', 'administration', 'would', 'make', 'good', 'on', 'a', 'u.s.', 'promise', 'to', 'donate', '$', '4', 'billion', 'to', 'the', 'global', 'vaccination', 'campaign', 'over', 'the', 'next', 'two', 'year', '.', 'mr.', 'biden', '’', 's', 'engagement', 'in', 'the', 'global', 'fight', 'against', 'the', 'pandemic', 'is', 'in', 'stark', 'contrast', 'to', 'the', 'approach', 'of', 'former', 'president', 'donald', 'j.', 'trump', ',', 'who', 'withdrew', 'the', 'united', 'states', 'from', 'the', 'world', 'health', 'organization', '.'], rawFeatures=SparseVector(200, {0: 1.0, 1: 1.0, 3: 1.0, 5: 3.0, 9: 1.0, 15: 1.0, 16: 2.0, 17: 18.0, 18: 1.0, 19: 1.0, 22: 2.0, 23: 2.0, 28: 5.0, 39: 1.0, 40: 3.0, 41: 1.0, 50: 2.0, 51: 1.0, 52: 2.0, 53: 2.0, 55: 2.0, 58: 1.0, 63: 3.0, 67: 9.0, 68: 1.0, 69: 1.0, 70: 1.0, 71: 3.0, 73: 1.0, 75: 2.0, 79: 1.0, 80: 3.0, 81: 1.0, 88: 5.0, 90: 2.0, 91: 3.0, 95: 6.0, 99: 4.0, 101: 1.0, 104: 1.0, 106: 3.0, 111: 1.0, 112: 1.0, 115: 3.0, 116: 1.0, 117: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 2.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 2.0, 133: 1.0, 135: 6.0, 136: 3.0, 144: 2.0, 151: 1.0, 152: 2.0, 153: 3.0, 154: 1.0, 155: 1.0, 156: 1.0, 159: 1.0, 160: 5.0, 163: 1.0, 164: 2.0, 166: 1.0, 168: 1.0, 169: 1.0, 171: 2.0, 173: 1.0, 174: 2.0, 175: 2.0, 180: 1.0, 181: 3.0, 182: 3.0, 184: 1.0, 185: 1.0, 195: 1.0, 196: 1.0}), features=SparseVector(200, {0: 0.6931, 1: 0.6931, 3: 0.4055, 5: 0.547, 9: 0.1823, 15: 0.6931, 16: 1.3863, 17: 0.0, 18: 0.6931, 19: 1.0986, 22: 1.3863, 23: 0.3646, 28: 0.0, 39: 1.0986, 40: 2.0794, 41: 0.4055, 50: 0.3646, 51: 0.0, 52: 2.1972, 53: 1.3863, 55: 0.3646, 58: 0.4055, 63: 0.0, 67: 0.0, 68: 1.0986, 69: 1.0986, 70: 0.6931, 71: 3.2958, 73: 0.6931, 75: 0.8109, 79: 0.4055, 80: 3.2958, 81: 0.6931, 88: 0.0, 90: 1.3863, 91: 0.0, 95: 0.0, 99: 0.7293, 101: 0.1823, 104: 0.1823, 106: 2.0794, 111: 0.6931, 112: 0.4055, 115: 2.0794, 116: 0.6931, 117: 0.6931, 120: 0.1823, 121: 0.0, 122: 0.1823, 123: 0.6931, 124: 0.4055, 125: 0.6931, 126: 0.4055, 127: 1.0986, 128: 1.3863, 129: 1.0986, 130: 0.6931, 131: 0.6931, 132: 2.1972, 133: 0.0, 135: 1.0939, 136: 0.0, 144: 0.3646, 151: 0.1823, 152: 0.8109, 153: 1.2164, 154: 0.6931, 155: 0.4055, 156: 1.0986, 159: 0.6931, 160: 2.0273, 163: 0.4055, 164: 0.3646, 166: 0.4055, 168: 0.6931, 169: 0.1823, 171: 0.8109, 173: 0.1823, 174: 1.3863, 175: 1.3863, 180: 0.6931, 181: 0.547, 182: 3.2958, 184: 0.6931, 185: 0.4055, 195: 0.6931, 196: 1.0986}))\n",
            "(200,[0,1,3,5,9,15,16,17,18,19,22,23,28,39,40,41,50,51,52,53,55,58,63,67,68,69,70,71,73,75,79,80,81,88,90,91,95,99,101,104,106,111,112,115,116,117,120,121,122,123,124,125,126,127,128,129,130,131,132,133,135,136,144,151,152,153,154,155,156,159,160,163,164,166,168,169,171,173,174,175,180,181,182,184,185,195,196],[1.0,1.0,1.0,3.0,1.0,1.0,2.0,18.0,1.0,1.0,2.0,2.0,5.0,1.0,3.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,3.0,9.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,3.0,1.0,5.0,2.0,3.0,6.0,4.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,6.0,3.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document=\"“ It ’ s a really important thing to do , to thank these Republicans who really took a stand against Trump and took chance that obviously have been challenging and will continue to be challenging , ” said Olivia Troye , co-director of the Republican Accountability Project . “ You 've seen them be censured by their own party in their own state and district , and the dynamic that ’ s come and the hatred and fury that you see from some of the supporter against them. ” Troye said that the organization is targeting voter on Fox News because “ there ’ s a whole voting population of Republican voter right now that are not OK with what happened ” a well a Trump voter for whom the Jan. 6 insurrection “ really gave them pause. ” The Republican Accountability Project said it will defend all of the House and Senate member who impeached or convicted Trump a part of it campaign , including those who are retiring or not up for reelection in 2022 .\", words=['“', 'it', '’', 's', 'a', 'really', 'important', 'thing', 'to', 'do', ',', 'to', 'thank', 'these', 'republicans', 'who', 'really', 'took', 'a', 'stand', 'against', 'trump', 'and', 'took', 'chance', 'that', 'obviously', 'have', 'been', 'challenging', 'and', 'will', 'continue', 'to', 'be', 'challenging', ',', '”', 'said', 'olivia', 'troye', ',', 'co-director', 'of', 'the', 'republican', 'accountability', 'project', '.', '“', 'you', \"'ve\", 'seen', 'them', 'be', 'censured', 'by', 'their', 'own', 'party', 'in', 'their', 'own', 'state', 'and', 'district', ',', 'and', 'the', 'dynamic', 'that', '’', 's', 'come', 'and', 'the', 'hatred', 'and', 'fury', 'that', 'you', 'see', 'from', 'some', 'of', 'the', 'supporter', 'against', 'them.', '”', 'troye', 'said', 'that', 'the', 'organization', 'is', 'targeting', 'voter', 'on', 'fox', 'news', 'because', '“', 'there', '’', 's', 'a', 'whole', 'voting', 'population', 'of', 'republican', 'voter', 'right', 'now', 'that', 'are', 'not', 'ok', 'with', 'what', 'happened', '”', 'a', 'well', 'a', 'trump', 'voter', 'for', 'whom', 'the', 'jan.', '6', 'insurrection', '“', 'really', 'gave', 'them', 'pause.', '”', 'the', 'republican', 'accountability', 'project', 'said', 'it', 'will', 'defend', 'all', 'of', 'the', 'house', 'and', 'senate', 'member', 'who', 'impeached', 'or', 'convicted', 'trump', 'a', 'part', 'of', 'it', 'campaign', ',', 'including', 'those', 'who', 'are', 'retiring', 'or', 'not', 'up', 'for', 'reelection', 'in', '2022', '.'], rawFeatures=SparseVector(200, {0: 1.0, 4: 1.0, 5: 5.0, 8: 1.0, 9: 5.0, 10: 1.0, 16: 3.0, 17: 9.0, 20: 3.0, 23: 1.0, 25: 1.0, 26: 1.0, 27: 1.0, 28: 2.0, 30: 3.0, 31: 1.0, 32: 1.0, 33: 1.0, 34: 2.0, 35: 2.0, 36: 1.0, 41: 1.0, 45: 1.0, 50: 1.0, 51: 1.0, 53: 1.0, 55: 2.0, 57: 2.0, 58: 2.0, 63: 3.0, 66: 1.0, 67: 11.0, 75: 1.0, 76: 1.0, 79: 3.0, 81: 1.0, 86: 6.0, 87: 1.0, 88: 3.0, 89: 1.0, 91: 7.0, 95: 5.0, 96: 1.0, 98: 1.0, 99: 1.0, 101: 3.0, 103: 2.0, 111: 1.0, 112: 6.0, 114: 2.0, 115: 4.0, 120: 4.0, 121: 1.0, 124: 2.0, 126: 1.0, 133: 2.0, 134: 1.0, 135: 1.0, 136: 1.0, 138: 2.0, 140: 1.0, 141: 2.0, 143: 1.0, 144: 2.0, 146: 1.0, 147: 2.0, 151: 1.0, 154: 1.0, 157: 2.0, 160: 5.0, 163: 2.0, 164: 2.0, 167: 3.0, 168: 1.0, 169: 2.0, 171: 4.0, 172: 1.0, 173: 1.0, 175: 1.0, 179: 1.0, 181: 1.0, 185: 1.0, 190: 2.0, 193: 1.0, 198: 1.0}), features=SparseVector(200, {0: 0.6931, 4: 0.4055, 5: 0.9116, 8: 0.6931, 9: 0.9116, 10: 1.0986, 16: 2.0794, 17: 0.0, 20: 0.547, 23: 0.1823, 25: 1.0986, 26: 0.6931, 27: 1.0986, 28: 0.0, 30: 1.2164, 31: 0.6931, 32: 0.6931, 33: 0.6931, 34: 1.3863, 35: 1.3863, 36: 1.0986, 41: 0.4055, 45: 0.6931, 50: 0.1823, 51: 0.0, 53: 0.6931, 55: 0.3646, 57: 1.3863, 58: 0.8109, 63: 0.0, 66: 0.6931, 67: 0.0, 75: 0.4055, 76: 0.6931, 79: 1.2164, 81: 0.6931, 86: 4.1589, 87: 0.6931, 88: 0.0, 89: 0.4055, 91: 0.0, 95: 0.0, 96: 0.6931, 98: 0.6931, 99: 0.1823, 101: 0.547, 103: 1.3863, 111: 0.6931, 112: 2.4328, 114: 0.8109, 115: 2.7726, 120: 0.7293, 121: 0.0, 124: 0.8109, 126: 0.4055, 133: 0.0, 134: 0.1823, 135: 0.1823, 136: 0.0, 138: 0.8109, 140: 0.4055, 141: 2.1972, 143: 0.6931, 144: 0.3646, 146: 0.4055, 147: 1.3863, 151: 0.1823, 154: 0.6931, 157: 0.8109, 160: 2.0273, 163: 0.8109, 164: 0.3646, 167: 0.547, 168: 0.6931, 169: 0.3646, 171: 1.6219, 172: 0.6931, 173: 0.1823, 175: 0.6931, 179: 0.6931, 181: 0.1823, 185: 0.4055, 190: 0.8109, 193: 1.0986, 198: 1.0986}))\n",
            "(200,[0,4,5,8,9,10,16,17,20,23,25,26,27,28,30,31,32,33,34,35,36,41,45,50,51,53,55,57,58,63,66,67,75,76,79,81,86,87,88,89,91,95,96,98,99,101,103,111,112,114,115,120,121,124,126,133,134,135,136,138,140,141,143,144,146,147,151,154,157,160,163,164,167,168,169,171,172,173,175,179,181,185,190,193,198],[1.0,1.0,5.0,1.0,5.0,1.0,3.0,9.0,3.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,3.0,1.0,11.0,1.0,1.0,3.0,1.0,6.0,1.0,3.0,1.0,7.0,5.0,1.0,1.0,1.0,3.0,2.0,1.0,6.0,2.0,4.0,4.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,5.0,2.0,2.0,3.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])\n",
            "Row(label=0.2, document=\"The protocol wa designed to protect the European Union 's single market without creating a land border on the island of Ireland . Leader Arlene Foster , Deputy Leader Nigel Dodds and member of parliament Jeffrey Donaldson and Sammy Wilson will join in the action challenging the protocol , a DUP statement said . They will be `` joining other likeminded unionist from across the United Kingdom a named party in judicial review proceeding challenging the Northern Ireland Protocol ’ s compatibility with Act of Union 1800 , the Northern Ireland Act of 1998 and the Belfast Agreement , '' the statement said .\", words=['the', 'protocol', 'wa', 'designed', 'to', 'protect', 'the', 'european', 'union', \"'s\", 'single', 'market', 'without', 'creating', 'a', 'land', 'border', 'on', 'the', 'island', 'of', 'ireland', '.', 'leader', 'arlene', 'foster', ',', 'deputy', 'leader', 'nigel', 'dodds', 'and', 'member', 'of', 'parliament', 'jeffrey', 'donaldson', 'and', 'sammy', 'wilson', 'will', 'join', 'in', 'the', 'action', 'challenging', 'the', 'protocol', ',', 'a', 'dup', 'statement', 'said', '.', 'they', 'will', 'be', '``', 'joining', 'other', 'likeminded', 'unionist', 'from', 'across', 'the', 'united', 'kingdom', 'a', 'named', 'party', 'in', 'judicial', 'review', 'proceeding', 'challenging', 'the', 'northern', 'ireland', 'protocol', '’', 's', 'compatibility', 'with', 'act', 'of', 'union', '1800', ',', 'the', 'northern', 'ireland', 'act', 'of', '1998', 'and', 'the', 'belfast', 'agreement', ',', \"''\", 'the', 'statement', 'said', '.'], rawFeatures=SparseVector(200, {7: 1.0, 8: 1.0, 13: 1.0, 17: 12.0, 20: 2.0, 23: 1.0, 24: 2.0, 28: 3.0, 33: 1.0, 35: 1.0, 37: 1.0, 43: 2.0, 48: 1.0, 50: 1.0, 51: 1.0, 55: 1.0, 57: 1.0, 60: 2.0, 63: 2.0, 67: 7.0, 74: 1.0, 79: 1.0, 88: 1.0, 89: 2.0, 91: 3.0, 93: 1.0, 95: 4.0, 101: 1.0, 104: 1.0, 107: 1.0, 108: 2.0, 120: 2.0, 121: 1.0, 122: 3.0, 126: 2.0, 128: 2.0, 131: 1.0, 133: 3.0, 134: 2.0, 135: 1.0, 136: 2.0, 140: 1.0, 145: 1.0, 146: 3.0, 148: 1.0, 149: 4.0, 152: 1.0, 164: 1.0, 166: 1.0, 167: 1.0, 170: 1.0, 171: 2.0, 179: 1.0, 183: 1.0, 185: 1.0, 188: 1.0, 190: 1.0, 195: 1.0}), features=SparseVector(200, {7: 0.6931, 8: 0.6931, 13: 1.0986, 17: 0.0, 20: 0.3646, 23: 0.1823, 24: 0.8109, 28: 0.0, 33: 0.6931, 35: 0.6931, 37: 0.6931, 43: 1.3863, 48: 0.6931, 50: 0.1823, 51: 0.0, 55: 0.1823, 57: 0.6931, 60: 1.3863, 63: 0.0, 67: 0.0, 74: 1.0986, 79: 0.4055, 88: 0.0, 89: 0.8109, 91: 0.0, 93: 0.4055, 95: 0.0, 101: 0.1823, 104: 0.1823, 107: 1.0986, 108: 2.1972, 120: 0.3646, 121: 0.0, 122: 0.547, 126: 0.8109, 128: 1.3863, 131: 0.6931, 133: 0.0, 134: 0.3646, 135: 0.1823, 136: 0.0, 140: 0.4055, 145: 0.6931, 146: 1.2164, 148: 0.4055, 149: 4.3944, 152: 0.4055, 164: 0.1823, 166: 0.4055, 167: 0.1823, 170: 0.6931, 171: 0.8109, 179: 0.6931, 183: 1.0986, 185: 0.4055, 188: 0.4055, 190: 0.4055, 195: 0.6931}))\n",
            "(200,[7,8,13,17,20,23,24,28,33,35,37,43,48,50,51,55,57,60,63,67,74,79,88,89,91,93,95,101,104,107,108,120,121,122,126,128,131,133,134,135,136,140,145,146,148,149,152,164,166,167,170,171,179,183,185,188,190,195],[1.0,1.0,1.0,12.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,7.0,1.0,1.0,1.0,2.0,3.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,2.0,2.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.3, document=\"Copper price climbed to their highest level in nearly a decade Friday a investor in the commodity anticipate rising demand for infrastructure and construction project in the post-pandemic economy . On Friday , the price of copper climbed above $ 4 per pound for the first time since September 2011 . The base metal is used in many construction material , including electrical wire and water pipe . The uptick is being driven by short and long term optimism for the commodity due to both strong expected demand and supply constraint . With the full reopening of the US economy on the horizon and President Joe Biden 's plan to invest heavily in America 's infrastructure , a well a China 's ongoing economic recovery , there is reason to believe copper demand will remain high .\", words=['copper', 'price', 'climbed', 'to', 'their', 'highest', 'level', 'in', 'nearly', 'a', 'decade', 'friday', 'a', 'investor', 'in', 'the', 'commodity', 'anticipate', 'rising', 'demand', 'for', 'infrastructure', 'and', 'construction', 'project', 'in', 'the', 'post-pandemic', 'economy', '.', 'on', 'friday', ',', 'the', 'price', 'of', 'copper', 'climbed', 'above', '$', '4', 'per', 'pound', 'for', 'the', 'first', 'time', 'since', 'september', '2011', '.', 'the', 'base', 'metal', 'is', 'used', 'in', 'many', 'construction', 'material', ',', 'including', 'electrical', 'wire', 'and', 'water', 'pipe', '.', 'the', 'uptick', 'is', 'being', 'driven', 'by', 'short', 'and', 'long', 'term', 'optimism', 'for', 'the', 'commodity', 'due', 'to', 'both', 'strong', 'expected', 'demand', 'and', 'supply', 'constraint', '.', 'with', 'the', 'full', 'reopening', 'of', 'the', 'us', 'economy', 'on', 'the', 'horizon', 'and', 'president', 'joe', 'biden', \"'s\", 'plan', 'to', 'invest', 'heavily', 'in', 'america', \"'s\", 'infrastructure', ',', 'a', 'well', 'a', 'china', \"'s\", 'ongoing', 'economic', 'recovery', ',', 'there', 'is', 'reason', 'to', 'believe', 'copper', 'demand', 'will', 'remain', 'high', '.'], rawFeatures=SparseVector(200, {3: 1.0, 4: 1.0, 5: 2.0, 9: 3.0, 17: 12.0, 20: 2.0, 24: 1.0, 26: 1.0, 28: 5.0, 30: 2.0, 31: 1.0, 32: 1.0, 37: 3.0, 47: 1.0, 50: 1.0, 51: 1.0, 56: 1.0, 60: 1.0, 62: 1.0, 63: 6.0, 64: 1.0, 66: 1.0, 67: 8.0, 70: 1.0, 72: 3.0, 73: 1.0, 77: 2.0, 87: 4.0, 88: 4.0, 91: 5.0, 93: 1.0, 95: 2.0, 97: 2.0, 98: 1.0, 99: 1.0, 104: 3.0, 106: 1.0, 114: 2.0, 116: 1.0, 121: 1.0, 122: 2.0, 123: 1.0, 130: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 2.0, 138: 2.0, 140: 1.0, 144: 5.0, 147: 1.0, 148: 1.0, 151: 1.0, 153: 3.0, 155: 1.0, 157: 2.0, 162: 1.0, 164: 2.0, 166: 1.0, 167: 1.0, 169: 1.0, 173: 1.0, 176: 1.0, 181: 1.0, 184: 1.0, 188: 6.0, 189: 1.0, 192: 1.0, 197: 1.0}), features=SparseVector(200, {3: 0.4055, 4: 0.4055, 5: 0.3646, 9: 0.547, 17: 0.0, 20: 0.3646, 24: 0.4055, 26: 0.6931, 28: 0.0, 30: 0.8109, 31: 0.6931, 32: 0.6931, 37: 2.0794, 47: 1.0986, 50: 0.1823, 51: 0.0, 56: 0.6931, 60: 0.6931, 62: 1.0986, 63: 0.0, 64: 0.6931, 66: 0.6931, 67: 0.0, 70: 0.6931, 72: 2.0794, 73: 0.6931, 77: 2.1972, 87: 2.7726, 88: 0.0, 91: 0.0, 93: 0.4055, 95: 0.0, 97: 1.3863, 98: 0.6931, 99: 0.1823, 104: 0.547, 106: 0.6931, 114: 0.8109, 116: 0.6931, 121: 0.0, 122: 0.3646, 123: 0.6931, 130: 0.6931, 133: 0.0, 134: 0.1823, 135: 0.1823, 136: 0.0, 138: 0.8109, 140: 0.4055, 144: 0.9116, 147: 0.6931, 148: 0.4055, 151: 0.1823, 153: 1.2164, 155: 0.4055, 157: 0.8109, 162: 0.6931, 164: 0.3646, 166: 0.4055, 167: 0.1823, 169: 0.1823, 173: 0.1823, 176: 1.0986, 181: 0.1823, 184: 0.6931, 188: 2.4328, 189: 1.0986, 192: 0.6931, 197: 1.0986}))\n",
            "(200,[3,4,5,9,17,20,24,26,28,30,31,32,37,47,50,51,56,60,62,63,64,66,67,70,72,73,77,87,88,91,93,95,97,98,99,104,106,114,116,121,122,123,130,133,134,135,136,138,140,144,147,148,151,153,155,157,162,164,166,167,169,173,176,181,184,188,189,192,197],[1.0,1.0,2.0,3.0,12.0,2.0,1.0,1.0,5.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,8.0,1.0,3.0,1.0,2.0,4.0,4.0,5.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,5.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0])\n",
            "Row(label=0.5, document=\"Many of the same thesis that bull have extolled for year continue to drive Bitcoin 's ascent . For example , fan still view it 21 million token limit a a hedge against the long-term devaluation of the U.S. dollar a the money supply increase . Enthusiasts also still value Bitcoin for it payment potential . A greater number of business are accepting cryptocurrency now than ever before , and Bitcoin 's network can settle payment transaction in an average of 10 minute . This compare to cross-border payment on traditional network , which can take up to a week to be validated and settled . The world 's most popular digital currency ha also received a boost from brand-name enterprise adoption . Electric-vehicle maker Tesla recently announced a $ 1.5 billion purchase of Bitcoin , which will be added to the automaker 's balance sheet . But you might be disappointed if you dig beyond the short-term cheerleading and technical analysis . History is clear that when next-big-thing investment go parabolic , they eventually implode . Furthermore , Bitcoin is full of flaw and misconception perpetuated on social medium platform .\", words=['many', 'of', 'the', 'same', 'thesis', 'that', 'bull', 'have', 'extolled', 'for', 'year', 'continue', 'to', 'drive', 'bitcoin', \"'s\", 'ascent', '.', 'for', 'example', ',', 'fan', 'still', 'view', 'it', '21', 'million', 'token', 'limit', 'a', 'a', 'hedge', 'against', 'the', 'long-term', 'devaluation', 'of', 'the', 'u.s.', 'dollar', 'a', 'the', 'money', 'supply', 'increase', '.', 'enthusiasts', 'also', 'still', 'value', 'bitcoin', 'for', 'it', 'payment', 'potential', '.', 'a', 'greater', 'number', 'of', 'business', 'are', 'accepting', 'cryptocurrency', 'now', 'than', 'ever', 'before', ',', 'and', 'bitcoin', \"'s\", 'network', 'can', 'settle', 'payment', 'transaction', 'in', 'an', 'average', 'of', '10', 'minute', '.', 'this', 'compare', 'to', 'cross-border', 'payment', 'on', 'traditional', 'network', ',', 'which', 'can', 'take', 'up', 'to', 'a', 'week', 'to', 'be', 'validated', 'and', 'settled', '.', 'the', 'world', \"'s\", 'most', 'popular', 'digital', 'currency', 'ha', 'also', 'received', 'a', 'boost', 'from', 'brand-name', 'enterprise', 'adoption', '.', 'electric-vehicle', 'maker', 'tesla', 'recently', 'announced', 'a', '$', '1.5', 'billion', 'purchase', 'of', 'bitcoin', ',', 'which', 'will', 'be', 'added', 'to', 'the', 'automaker', \"'s\", 'balance', 'sheet', '.', 'but', 'you', 'might', 'be', 'disappointed', 'if', 'you', 'dig', 'beyond', 'the', 'short-term', 'cheerleading', 'and', 'technical', 'analysis', '.', 'history', 'is', 'clear', 'that', 'when', 'next-big-thing', 'investment', 'go', 'parabolic', ',', 'they', 'eventually', 'implode', '.', 'furthermore', ',', 'bitcoin', 'is', 'full', 'of', 'flaw', 'and', 'misconception', 'perpetuated', 'on', 'social', 'medium', 'platform', '.'], rawFeatures=SparseVector(200, {1: 2.0, 2: 1.0, 3: 1.0, 4: 3.0, 5: 1.0, 6: 1.0, 7: 3.0, 9: 2.0, 11: 1.0, 15: 1.0, 17: 7.0, 18: 5.0, 20: 1.0, 21: 1.0, 22: 1.0, 23: 2.0, 24: 3.0, 28: 10.0, 30: 1.0, 34: 1.0, 40: 1.0, 41: 1.0, 42: 1.0, 43: 1.0, 45: 1.0, 46: 1.0, 48: 1.0, 51: 2.0, 55: 4.0, 56: 1.0, 58: 2.0, 61: 1.0, 63: 1.0, 64: 2.0, 67: 13.0, 72: 1.0, 75: 1.0, 76: 1.0, 86: 3.0, 88: 5.0, 89: 1.0, 90: 2.0, 91: 5.0, 92: 1.0, 93: 1.0, 95: 6.0, 96: 1.0, 97: 1.0, 99: 2.0, 100: 1.0, 101: 1.0, 103: 1.0, 104: 1.0, 112: 3.0, 114: 2.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 2.0, 121: 3.0, 122: 1.0, 124: 1.0, 125: 1.0, 133: 2.0, 134: 1.0, 136: 3.0, 138: 3.0, 143: 2.0, 144: 4.0, 145: 1.0, 146: 2.0, 148: 1.0, 151: 2.0, 152: 2.0, 153: 2.0, 155: 2.0, 157: 1.0, 159: 1.0, 160: 3.0, 161: 3.0, 162: 1.0, 163: 2.0, 165: 1.0, 167: 1.0, 169: 2.0, 170: 1.0, 172: 1.0, 173: 1.0, 174: 2.0, 177: 1.0, 180: 1.0, 181: 1.0, 188: 5.0, 190: 1.0, 192: 2.0, 194: 1.0, 199: 1.0}), features=SparseVector(200, {1: 1.3863, 2: 1.0986, 3: 0.4055, 4: 1.2164, 5: 0.1823, 6: 1.0986, 7: 2.0794, 9: 0.3646, 11: 1.0986, 15: 0.6931, 17: 0.0, 18: 3.4657, 20: 0.1823, 21: 1.0986, 22: 0.6931, 23: 0.3646, 24: 1.2164, 28: 0.0, 30: 0.4055, 34: 0.6931, 40: 0.6931, 41: 0.4055, 42: 1.0986, 43: 0.6931, 45: 0.6931, 46: 1.0986, 48: 0.6931, 51: 0.0, 55: 0.7293, 56: 0.6931, 58: 0.8109, 61: 1.0986, 63: 0.0, 64: 1.3863, 67: 0.0, 72: 0.6931, 75: 0.4055, 76: 0.6931, 86: 2.0794, 88: 0.0, 89: 0.4055, 90: 1.3863, 91: 0.0, 92: 1.0986, 93: 0.4055, 95: 0.0, 96: 0.6931, 97: 0.6931, 99: 0.3646, 100: 1.0986, 101: 0.1823, 103: 0.6931, 104: 0.1823, 112: 1.2164, 114: 0.8109, 117: 0.6931, 118: 1.0986, 119: 1.0986, 120: 0.3646, 121: 0.0, 122: 0.1823, 124: 0.4055, 125: 0.6931, 133: 0.0, 134: 0.1823, 136: 0.0, 138: 1.2164, 143: 1.3863, 144: 0.7293, 145: 0.6931, 146: 0.8109, 148: 0.4055, 151: 0.3646, 152: 0.8109, 153: 0.8109, 155: 0.8109, 157: 0.4055, 159: 0.6931, 160: 1.2164, 161: 3.2958, 162: 0.6931, 163: 0.8109, 165: 1.0986, 167: 0.1823, 169: 0.3646, 170: 0.6931, 172: 0.6931, 173: 0.1823, 174: 1.3863, 177: 1.0986, 180: 0.6931, 181: 0.1823, 188: 2.0273, 190: 0.4055, 192: 1.3863, 194: 1.0986, 199: 1.0986}))\n",
            "(200,[1,2,3,4,5,6,7,9,11,15,17,18,20,21,22,23,24,28,30,34,40,41,42,43,45,46,48,51,55,56,58,61,63,64,67,72,75,76,86,88,89,90,91,92,93,95,96,97,99,100,101,103,104,112,114,117,118,119,120,121,122,124,125,133,134,136,138,143,144,145,146,148,151,152,153,155,157,159,160,161,162,163,165,167,169,170,172,173,174,177,180,181,188,190,192,194,199],[2.0,1.0,1.0,3.0,1.0,1.0,3.0,2.0,1.0,1.0,7.0,5.0,1.0,1.0,1.0,2.0,3.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,2.0,1.0,1.0,2.0,13.0,1.0,1.0,1.0,3.0,5.0,1.0,2.0,5.0,1.0,1.0,6.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,2.0,4.0,1.0,2.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,3.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqLfGmO0t7sv"
      },
      "source": [
        "## c. Try with NGrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6efSH7Tmt_UY",
        "outputId": "cdffb52d-b1ce-421b-b3bd-c408f3acb30d"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "        (0.0, doc1.split(' ')),\r\n",
        "        (0.1, doc2.split(' ')),\r\n",
        "        (0.2, doc3.split(' ')),\r\n",
        "        (0.3, doc4.split(' ')),\r\n",
        "        (0.5, doc5.split(' '))\r\n",
        "    ], [\"label\", \"document\"])\r\n",
        "\r\n",
        "\r\n",
        "ngram = NGram(n=2, inputCol=\"document\", outputCol=\"ngrams\")\r\n",
        "\r\n",
        "ngramDataFrame = ngram.transform(documentData)\r\n",
        "\r\n",
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(ngramDataFrame)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "\r\n",
        "\r\n",
        "print(\"TF-IDF with ngram:\")\r\n",
        "for each in tfidf.collect():\r\n",
        "    print(each)\r\n",
        "    print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[6,7,11,15,1...|\n",
            "|  0.1|(200,[0,3,5,6,11,...|\n",
            "|  0.2|(200,[4,20,21,22,...|\n",
            "|  0.3|(200,[0,1,2,3,4,6...|\n",
            "|  0.5|(200,[0,1,2,4,5,6...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF without NLP:\n",
            "Row(label=0.0, document=['In', 'his', 'first', 'trips', 'as', 'president,', 'President', 'Biden', 'traveled', 'to', 'Wisconsin', 'and', 'Michigan', 'to', 'promote', 'his', 'vaccination', 'rollout', 'plan', 'and', 'the', '$1.9', 'trillion', 'relief', 'bill', 'he', 'hopes', 'can', 'restore', 'the', 'American', 'economy.\\n\\nAfter', 'an', 'optimistic', 'vow', 'on', 'Tuesday', 'that', 'any', 'American', 'who', 'wanted', 'a', 'vaccine', '“could', 'have', 'one', 'by', 'the', 'end', 'of', 'July', 'this', 'year,”', 'Mr.', 'Biden', 'was', 'asking', 'for', 'patience', 'on', 'Friday,', 'saying', 'the', 'United', 'States', 'could', 'be“approaching', 'normalcy”', 'by', 'the', 'end', 'of', 'the', 'year.', 'The', 'week', 'ended', 'as', 'winter', 'storms', 'hitting', 'much', 'of', 'the', 'country', 'delayed', 'the', 'delivery', 'of', 'six', 'million', 'vaccines.\\n\\nMr.', 'Biden', 'addressed', 'the', 'virtual', 'G7', 'summit', 'and', 'said', 'that', 'his', 'administration', 'would', 'make', 'good', 'on', 'a', 'U.S.', 'promise', 'to', 'donate', '$4', 'billion', 'to', 'the', 'global', 'vaccination', 'campaign', 'over', 'the', 'next', 'two', 'years.', 'Mr.', 'Biden’s', 'engagement', 'in', 'the', 'global', 'fight', 'against', 'the', 'pandemic', 'is', 'in', 'stark', 'contrast', 'to', 'the', 'approach', 'of', 'former', 'President', 'Donald', 'J.', 'Trump,', 'who', 'withdrew', 'the', 'United', 'States', 'from', 'the', 'World', 'Health', 'Organization.'], ngrams=['In his', 'his first', 'first trips', 'trips as', 'as president,', 'president, President', 'President Biden', 'Biden traveled', 'traveled to', 'to Wisconsin', 'Wisconsin and', 'and Michigan', 'Michigan to', 'to promote', 'promote his', 'his vaccination', 'vaccination rollout', 'rollout plan', 'plan and', 'and the', 'the $1.9', '$1.9 trillion', 'trillion relief', 'relief bill', 'bill he', 'he hopes', 'hopes can', 'can restore', 'restore the', 'the American', 'American economy.\\n\\nAfter', 'economy.\\n\\nAfter an', 'an optimistic', 'optimistic vow', 'vow on', 'on Tuesday', 'Tuesday that', 'that any', 'any American', 'American who', 'who wanted', 'wanted a', 'a vaccine', 'vaccine “could', '“could have', 'have one', 'one by', 'by the', 'the end', 'end of', 'of July', 'July this', 'this year,”', 'year,” Mr.', 'Mr. Biden', 'Biden was', 'was asking', 'asking for', 'for patience', 'patience on', 'on Friday,', 'Friday, saying', 'saying the', 'the United', 'United States', 'States could', 'could be“approaching', 'be“approaching normalcy”', 'normalcy” by', 'by the', 'the end', 'end of', 'of the', 'the year.', 'year. The', 'The week', 'week ended', 'ended as', 'as winter', 'winter storms', 'storms hitting', 'hitting much', 'much of', 'of the', 'the country', 'country delayed', 'delayed the', 'the delivery', 'delivery of', 'of six', 'six million', 'million vaccines.\\n\\nMr.', 'vaccines.\\n\\nMr. Biden', 'Biden addressed', 'addressed the', 'the virtual', 'virtual G7', 'G7 summit', 'summit and', 'and said', 'said that', 'that his', 'his administration', 'administration would', 'would make', 'make good', 'good on', 'on a', 'a U.S.', 'U.S. promise', 'promise to', 'to donate', 'donate $4', '$4 billion', 'billion to', 'to the', 'the global', 'global vaccination', 'vaccination campaign', 'campaign over', 'over the', 'the next', 'next two', 'two years.', 'years. Mr.', 'Mr. Biden’s', 'Biden’s engagement', 'engagement in', 'in the', 'the global', 'global fight', 'fight against', 'against the', 'the pandemic', 'pandemic is', 'is in', 'in stark', 'stark contrast', 'contrast to', 'to the', 'the approach', 'approach of', 'of former', 'former President', 'President Donald', 'Donald J.', 'J. Trump,', 'Trump, who', 'who withdrew', 'withdrew the', 'the United', 'United States', 'States from', 'from the', 'the World', 'World Health', 'Health Organization.'], rawFeatures=SparseVector(200, {6: 1.0, 7: 2.0, 11: 1.0, 15: 1.0, 16: 2.0, 17: 2.0, 19: 1.0, 23: 1.0, 24: 1.0, 25: 3.0, 27: 2.0, 28: 1.0, 31: 2.0, 32: 1.0, 33: 1.0, 37: 1.0, 38: 2.0, 39: 2.0, 40: 2.0, 44: 2.0, 46: 1.0, 47: 1.0, 50: 1.0, 51: 1.0, 52: 1.0, 53: 2.0, 55: 1.0, 56: 2.0, 57: 1.0, 59: 1.0, 60: 1.0, 63: 3.0, 64: 2.0, 65: 1.0, 67: 1.0, 68: 2.0, 70: 1.0, 71: 2.0, 75: 1.0, 76: 1.0, 77: 1.0, 85: 2.0, 86: 2.0, 87: 1.0, 89: 2.0, 90: 2.0, 94: 1.0, 95: 2.0, 96: 2.0, 98: 1.0, 99: 2.0, 101: 2.0, 104: 2.0, 105: 1.0, 110: 1.0, 111: 3.0, 112: 1.0, 113: 2.0, 114: 1.0, 115: 1.0, 116: 1.0, 119: 2.0, 120: 1.0, 121: 2.0, 126: 2.0, 128: 1.0, 129: 4.0, 131: 1.0, 132: 2.0, 133: 1.0, 135: 4.0, 138: 1.0, 140: 1.0, 145: 1.0, 148: 1.0, 151: 1.0, 153: 2.0, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 2.0, 163: 2.0, 164: 1.0, 165: 2.0, 167: 1.0, 170: 3.0, 174: 2.0, 175: 2.0, 179: 1.0, 180: 2.0, 182: 4.0, 183: 1.0, 184: 2.0, 185: 1.0, 186: 2.0, 190: 1.0, 191: 1.0, 192: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 199: 1.0}), features=SparseVector(200, {6: 0.1823, 7: 1.3863, 11: 0.4055, 15: 0.4055, 16: 0.8109, 17: 1.3863, 19: 0.6931, 23: 0.1823, 24: 0.6931, 25: 0.547, 27: 0.8109, 28: 0.6931, 31: 0.8109, 32: 0.6931, 33: 0.4055, 37: 0.6931, 38: 0.8109, 39: 0.3646, 40: 0.8109, 44: 0.3646, 46: 0.6931, 47: 0.1823, 50: 0.4055, 51: 0.6931, 52: 0.4055, 53: 2.1972, 55: 0.4055, 56: 0.3646, 57: 0.6931, 59: 0.1823, 60: 0.4055, 63: 0.547, 64: 2.1972, 65: 0.4055, 67: 0.4055, 68: 0.3646, 70: 0.6931, 71: 0.8109, 75: 0.1823, 76: 0.4055, 77: 0.4055, 85: 0.0, 86: 0.3646, 87: 0.6931, 89: 0.8109, 90: 0.8109, 94: 0.0, 95: 0.3646, 96: 0.8109, 98: 0.1823, 99: 0.8109, 101: 0.8109, 104: 1.3863, 105: 0.4055, 110: 0.0, 111: 2.0794, 112: 0.4055, 113: 1.3863, 114: 0.6931, 115: 0.1823, 116: 1.0986, 119: 0.8109, 120: 0.1823, 121: 0.8109, 126: 0.8109, 128: 0.4055, 129: 2.7726, 131: 1.0986, 132: 0.8109, 133: 0.6931, 135: 1.6219, 138: 1.0986, 140: 0.1823, 145: 0.1823, 148: 0.6931, 151: 0.6931, 153: 0.8109, 154: 0.4055, 155: 0.4055, 156: 0.6931, 157: 0.6931, 158: 1.3863, 163: 0.8109, 164: 0.1823, 165: 0.8109, 167: 0.0, 170: 0.547, 174: 0.8109, 175: 0.3646, 179: 0.4055, 180: 0.0, 182: 0.7293, 183: 0.1823, 184: 1.3863, 185: 0.4055, 186: 0.8109, 190: 0.1823, 191: 0.4055, 192: 1.0986, 194: 0.4055, 195: 0.4055, 196: 0.4055, 199: 0.6931}))\n",
            "(200,[6,7,11,15,16,17,19,23,24,25,27,28,31,32,33,37,38,39,40,44,46,47,50,51,52,53,55,56,57,59,60,63,64,65,67,68,70,71,75,76,77,85,86,87,89,90,94,95,96,98,99,101,104,105,110,111,112,113,114,115,116,119,120,121,126,128,129,131,132,133,135,138,140,145,148,151,153,154,155,156,157,158,163,164,165,167,170,174,175,179,180,182,183,184,185,186,190,191,192,194,195,196,199],[1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,2.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,4.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,3.0,2.0,2.0,1.0,2.0,4.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
            "Row(label=0.1, document=['“It’s', 'a', 'really', 'important', 'thing', 'to', 'do,', 'to', 'thank', 'these', 'Republicans', 'who', 'really', 'took', 'a', 'stand', 'against', 'Trump', 'and', 'took', 'chances', 'that', 'obviously', 'have', 'been', 'challenging', 'and', 'will', 'continue', 'to', 'be', 'challenging,”', 'said', 'Olivia', 'Troye,', 'co-director', 'of', 'the', 'Republican', 'Accountability', 'Project.', \"“You've\", 'seen', 'them', 'be', 'censured', 'by', 'their', 'own', 'parties', 'in', 'their', 'own', 'states', 'and', 'districts,', 'and', 'the', 'dynamic', 'that’s', 'come', 'and', 'the', 'hatred', 'and', 'fury', 'that', 'you', 'see', 'from', 'some', 'of', 'the', 'supporters', 'against', 'them.”\\n\\nTroye', 'said', 'that', 'the', 'organization', 'is', 'targeting', 'voters', 'on', 'Fox', 'News', 'because', '“there’s', 'a', 'whole', 'voting', 'population', 'of', 'Republican', 'voters', 'right', 'now', 'that', 'are', 'not', 'OK', 'with', 'what', 'happened”', 'as', 'well', 'as', 'Trump', 'voters', 'for', 'whom', 'the', 'Jan.', '6', 'insurrection', '“really', 'gave', 'them', 'pause.”\\n\\nThe', 'Republican', 'Accountability', 'Project', 'said', 'it', 'will', 'defend', 'all', 'of', 'the', 'House', 'and', 'Senate', 'members', 'who', 'impeached', 'or', 'convicted', 'Trump', 'as', 'part', 'of', 'its', 'campaign,', 'including', 'those', 'who', 'are', 'retiring', 'or', 'not', 'up', 'for', 'reelection', 'in', '2022.'], ngrams=['“It’s a', 'a really', 'really important', 'important thing', 'thing to', 'to do,', 'do, to', 'to thank', 'thank these', 'these Republicans', 'Republicans who', 'who really', 'really took', 'took a', 'a stand', 'stand against', 'against Trump', 'Trump and', 'and took', 'took chances', 'chances that', 'that obviously', 'obviously have', 'have been', 'been challenging', 'challenging and', 'and will', 'will continue', 'continue to', 'to be', 'be challenging,”', 'challenging,” said', 'said Olivia', 'Olivia Troye,', 'Troye, co-director', 'co-director of', 'of the', 'the Republican', 'Republican Accountability', 'Accountability Project.', \"Project. “You've\", \"“You've seen\", 'seen them', 'them be', 'be censured', 'censured by', 'by their', 'their own', 'own parties', 'parties in', 'in their', 'their own', 'own states', 'states and', 'and districts,', 'districts, and', 'and the', 'the dynamic', 'dynamic that’s', 'that’s come', 'come and', 'and the', 'the hatred', 'hatred and', 'and fury', 'fury that', 'that you', 'you see', 'see from', 'from some', 'some of', 'of the', 'the supporters', 'supporters against', 'against them.”\\n\\nTroye', 'them.”\\n\\nTroye said', 'said that', 'that the', 'the organization', 'organization is', 'is targeting', 'targeting voters', 'voters on', 'on Fox', 'Fox News', 'News because', 'because “there’s', '“there’s a', 'a whole', 'whole voting', 'voting population', 'population of', 'of Republican', 'Republican voters', 'voters right', 'right now', 'now that', 'that are', 'are not', 'not OK', 'OK with', 'with what', 'what happened”', 'happened” as', 'as well', 'well as', 'as Trump', 'Trump voters', 'voters for', 'for whom', 'whom the', 'the Jan.', 'Jan. 6', '6 insurrection', 'insurrection “really', '“really gave', 'gave them', 'them pause.”\\n\\nThe', 'pause.”\\n\\nThe Republican', 'Republican Accountability', 'Accountability Project', 'Project said', 'said it', 'it will', 'will defend', 'defend all', 'all of', 'of the', 'the House', 'House and', 'and Senate', 'Senate members', 'members who', 'who impeached', 'impeached or', 'or convicted', 'convicted Trump', 'Trump as', 'as part', 'part of', 'of its', 'its campaign,', 'campaign, including', 'including those', 'those who', 'who are', 'are retiring', 'retiring or', 'or not', 'not up', 'up for', 'for reelection', 'reelection in', 'in 2022.'], rawFeatures=SparseVector(200, {0: 2.0, 3: 1.0, 5: 1.0, 6: 1.0, 11: 3.0, 13: 1.0, 16: 2.0, 17: 1.0, 19: 2.0, 20: 2.0, 22: 1.0, 23: 2.0, 25: 1.0, 26: 3.0, 29: 2.0, 30: 2.0, 32: 1.0, 33: 1.0, 34: 2.0, 38: 1.0, 39: 1.0, 40: 1.0, 43: 1.0, 45: 1.0, 47: 3.0, 48: 1.0, 49: 1.0, 52: 2.0, 54: 2.0, 56: 3.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 2.0, 63: 2.0, 68: 1.0, 70: 1.0, 75: 2.0, 80: 1.0, 81: 1.0, 83: 1.0, 85: 1.0, 86: 1.0, 88: 2.0, 89: 1.0, 91: 1.0, 93: 1.0, 94: 3.0, 95: 3.0, 98: 2.0, 100: 2.0, 101: 2.0, 102: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 1.0, 108: 1.0, 109: 2.0, 110: 1.0, 112: 2.0, 117: 1.0, 118: 3.0, 120: 1.0, 121: 1.0, 128: 1.0, 130: 1.0, 132: 1.0, 135: 1.0, 136: 3.0, 140: 2.0, 142: 1.0, 143: 2.0, 144: 1.0, 145: 2.0, 146: 1.0, 148: 1.0, 149: 2.0, 150: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 160: 1.0, 161: 2.0, 163: 3.0, 165: 1.0, 166: 1.0, 167: 2.0, 168: 1.0, 170: 1.0, 173: 1.0, 175: 2.0, 177: 1.0, 178: 1.0, 180: 1.0, 182: 1.0, 183: 2.0, 184: 1.0, 185: 1.0, 186: 2.0, 187: 1.0, 190: 1.0, 191: 1.0, 194: 1.0, 195: 3.0, 199: 1.0}), features=SparseVector(200, {0: 0.8109, 3: 0.6931, 5: 0.6931, 6: 0.1823, 11: 1.2164, 13: 0.6931, 16: 0.8109, 17: 0.6931, 19: 1.3863, 20: 0.8109, 22: 0.4055, 23: 0.3646, 25: 0.1823, 26: 1.2164, 29: 0.8109, 30: 0.8109, 32: 0.6931, 33: 0.4055, 34: 0.8109, 38: 0.4055, 39: 0.1823, 40: 0.4055, 43: 0.4055, 45: 0.4055, 47: 0.547, 48: 0.4055, 49: 0.6931, 52: 0.8109, 54: 1.3863, 56: 0.547, 59: 0.1823, 60: 0.4055, 61: 0.6931, 62: 1.3863, 63: 0.3646, 68: 0.1823, 70: 0.6931, 75: 0.3646, 80: 0.4055, 81: 0.6931, 83: 0.4055, 85: 0.0, 86: 0.1823, 88: 0.8109, 89: 0.4055, 91: 0.1823, 93: 0.4055, 94: 0.0, 95: 0.547, 98: 0.3646, 100: 2.1972, 101: 0.8109, 102: 0.4055, 104: 0.6931, 105: 0.4055, 106: 0.6931, 107: 0.6931, 108: 0.6931, 109: 0.8109, 110: 0.0, 112: 0.8109, 117: 0.6931, 118: 3.2958, 120: 0.1823, 121: 0.4055, 128: 0.4055, 130: 0.4055, 132: 0.4055, 135: 0.4055, 136: 1.2164, 140: 0.3646, 142: 0.4055, 143: 1.3863, 144: 0.6931, 145: 0.3646, 146: 0.4055, 148: 0.6931, 149: 0.3646, 150: 0.4055, 153: 0.4055, 154: 0.4055, 155: 0.4055, 160: 1.0986, 161: 1.3863, 163: 1.2164, 165: 0.4055, 166: 1.0986, 167: 0.0, 168: 0.4055, 170: 0.1823, 173: 1.0986, 175: 0.3646, 177: 0.4055, 178: 0.4055, 180: 0.0, 182: 0.1823, 183: 0.3646, 184: 0.6931, 185: 0.4055, 186: 0.8109, 187: 0.4055, 190: 0.1823, 191: 0.4055, 194: 0.4055, 195: 1.2164, 199: 0.6931}))\n",
            "(200,[0,3,5,6,11,13,16,17,19,20,22,23,25,26,29,30,32,33,34,38,39,40,43,45,47,48,49,52,54,56,59,60,61,62,63,68,70,75,80,81,83,85,86,88,89,91,93,94,95,98,100,101,102,104,105,106,107,108,109,110,112,117,118,120,121,128,130,132,135,136,140,142,143,144,145,146,148,149,150,153,154,155,160,161,163,165,166,167,168,170,173,175,177,178,180,182,183,184,185,186,187,190,191,194,195,199],[2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,3.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,3.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0])\n",
            "Row(label=0.2, document=['The', 'protocol', 'was', 'designed', 'to', 'protect', 'the', 'European', \"Union's\", 'single', 'market', 'without', 'creating', 'a', 'land', 'border', 'on', 'the', 'island', 'of', 'Ireland.\\n\\nLeader', 'Arlene', 'Foster,', 'Deputy', 'Leader', 'Nigel', 'Dodds', 'and', 'members', 'of', 'parliament', 'Jeffrey', 'Donaldson', 'and', 'Sammy', 'Wilson', 'will', 'join', 'in', 'the', 'action', 'challenging', 'the', 'protocol,', 'a', 'DUP', 'statement', 'said.\\n\\nThey', 'will', 'be', '\"joining', 'other', 'likeminded', 'unionists', 'from', 'across', 'the', 'United', 'Kingdom', 'as', 'named', 'parties', 'in', 'judicial', 'review', 'proceedings', 'challenging', 'the', 'Northern', 'Ireland', 'Protocol’s', 'compatibility', 'with', 'Act', 'of', 'Union', '1800,', 'the', 'Northern', 'Ireland', 'Act', 'of', '1998', 'and', 'the', 'Belfast', 'Agreement,\"', 'the', 'statement', 'said.'], ngrams=['The protocol', 'protocol was', 'was designed', 'designed to', 'to protect', 'protect the', 'the European', \"European Union's\", \"Union's single\", 'single market', 'market without', 'without creating', 'creating a', 'a land', 'land border', 'border on', 'on the', 'the island', 'island of', 'of Ireland.\\n\\nLeader', 'Ireland.\\n\\nLeader Arlene', 'Arlene Foster,', 'Foster, Deputy', 'Deputy Leader', 'Leader Nigel', 'Nigel Dodds', 'Dodds and', 'and members', 'members of', 'of parliament', 'parliament Jeffrey', 'Jeffrey Donaldson', 'Donaldson and', 'and Sammy', 'Sammy Wilson', 'Wilson will', 'will join', 'join in', 'in the', 'the action', 'action challenging', 'challenging the', 'the protocol,', 'protocol, a', 'a DUP', 'DUP statement', 'statement said.\\n\\nThey', 'said.\\n\\nThey will', 'will be', 'be \"joining', '\"joining other', 'other likeminded', 'likeminded unionists', 'unionists from', 'from across', 'across the', 'the United', 'United Kingdom', 'Kingdom as', 'as named', 'named parties', 'parties in', 'in judicial', 'judicial review', 'review proceedings', 'proceedings challenging', 'challenging the', 'the Northern', 'Northern Ireland', 'Ireland Protocol’s', 'Protocol’s compatibility', 'compatibility with', 'with Act', 'Act of', 'of Union', 'Union 1800,', '1800, the', 'the Northern', 'Northern Ireland', 'Ireland Act', 'Act of', 'of 1998', '1998 and', 'and the', 'the Belfast', 'Belfast Agreement,\"', 'Agreement,\" the', 'the statement', 'statement said.'], rawFeatures=SparseVector(200, {4: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 23: 3.0, 25: 2.0, 28: 1.0, 30: 2.0, 31: 1.0, 33: 2.0, 34: 2.0, 37: 1.0, 39: 2.0, 40: 1.0, 43: 1.0, 44: 1.0, 47: 1.0, 50: 1.0, 51: 1.0, 52: 2.0, 57: 2.0, 59: 1.0, 65: 1.0, 71: 2.0, 74: 1.0, 77: 3.0, 80: 2.0, 83: 1.0, 84: 1.0, 85: 1.0, 86: 1.0, 91: 1.0, 93: 1.0, 94: 1.0, 99: 1.0, 109: 1.0, 110: 1.0, 115: 4.0, 117: 1.0, 120: 2.0, 121: 1.0, 122: 1.0, 123: 1.0, 126: 1.0, 127: 1.0, 133: 1.0, 134: 2.0, 140: 1.0, 141: 1.0, 142: 2.0, 146: 2.0, 149: 2.0, 150: 3.0, 164: 1.0, 167: 1.0, 171: 1.0, 174: 1.0, 177: 2.0, 179: 1.0, 180: 1.0, 182: 1.0, 187: 2.0, 190: 1.0, 196: 1.0}), features=SparseVector(200, {4: 0.4055, 20: 0.4055, 21: 0.6931, 22: 0.4055, 23: 0.547, 25: 0.3646, 28: 0.6931, 30: 0.8109, 31: 0.4055, 33: 0.8109, 34: 0.8109, 37: 0.6931, 39: 0.3646, 40: 0.4055, 43: 0.4055, 44: 0.1823, 47: 0.1823, 50: 0.4055, 51: 0.6931, 52: 0.8109, 57: 1.3863, 59: 0.1823, 65: 0.4055, 71: 0.8109, 74: 1.0986, 77: 1.2164, 80: 0.8109, 83: 0.4055, 84: 1.0986, 85: 0.0, 86: 0.1823, 91: 0.1823, 93: 0.4055, 94: 0.0, 99: 0.4055, 109: 0.4055, 110: 0.0, 115: 0.7293, 117: 0.6931, 120: 0.3646, 121: 0.4055, 122: 1.0986, 123: 0.6931, 126: 0.4055, 127: 0.6931, 133: 0.6931, 134: 1.3863, 140: 0.1823, 141: 0.4055, 142: 0.8109, 146: 0.8109, 149: 0.3646, 150: 1.2164, 164: 0.1823, 167: 0.0, 171: 0.6931, 174: 0.4055, 177: 0.8109, 179: 0.4055, 180: 0.0, 182: 0.1823, 187: 0.8109, 190: 0.1823, 196: 0.4055}))\n",
            "(200,[4,20,21,22,23,25,28,30,31,33,34,37,39,40,43,44,47,50,51,52,57,59,65,71,74,77,80,83,84,85,86,91,93,94,99,109,110,115,117,120,121,122,123,126,127,133,134,140,141,142,146,149,150,164,167,171,174,177,179,180,182,187,190,196],[1.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0])\n",
            "Row(label=0.3, document=['Copper', 'prices', 'climbed', 'to', 'their', 'highest', 'level', 'in', 'nearly', 'a', 'decade', 'Friday', 'as', 'investors', 'in', 'the', 'commodity', 'anticipate', 'rising', 'demand', 'for', 'infrastructure', 'and', 'construction', 'projects', 'in', 'the', 'post-pandemic', 'economy.\\n\\nOn', 'Friday,', 'the', 'price', 'of', 'copper', 'climbed', 'above', '$4', 'per', 'pound', 'for', 'the', 'first', 'time', 'since', 'September', '2011.\\nThe', 'base', 'metal', 'is', 'used', 'in', 'many', 'construction', 'materials,', 'including', 'electrical', 'wires', 'and', 'water', 'pipes.', 'The', 'uptick', 'is', 'being', 'driven', 'by', 'short', 'and', 'long', 'term', 'optimism', 'for', 'the', 'commodity', 'due', 'to', 'both', 'strong', 'expected', 'demand', 'and', 'supply', 'constraints.\\nWith', 'the', 'full', 'reopening', 'of', 'the', 'US', 'economy', 'on', 'the', 'horizon', 'and', 'President', 'Joe', \"Biden's\", 'plan', 'to', 'invest', 'heavily', 'in', \"America's\", 'infrastructure,', 'as', 'well', 'as', \"China's\", 'ongoing', 'economic', 'recovery,', 'there', 'is', 'reason', 'to', 'believe', 'copper', 'demand', 'will', 'remain', 'high.'], ngrams=['Copper prices', 'prices climbed', 'climbed to', 'to their', 'their highest', 'highest level', 'level in', 'in nearly', 'nearly a', 'a decade', 'decade Friday', 'Friday as', 'as investors', 'investors in', 'in the', 'the commodity', 'commodity anticipate', 'anticipate rising', 'rising demand', 'demand for', 'for infrastructure', 'infrastructure and', 'and construction', 'construction projects', 'projects in', 'in the', 'the post-pandemic', 'post-pandemic economy.\\n\\nOn', 'economy.\\n\\nOn Friday,', 'Friday, the', 'the price', 'price of', 'of copper', 'copper climbed', 'climbed above', 'above $4', '$4 per', 'per pound', 'pound for', 'for the', 'the first', 'first time', 'time since', 'since September', 'September 2011.\\nThe', '2011.\\nThe base', 'base metal', 'metal is', 'is used', 'used in', 'in many', 'many construction', 'construction materials,', 'materials, including', 'including electrical', 'electrical wires', 'wires and', 'and water', 'water pipes.', 'pipes. The', 'The uptick', 'uptick is', 'is being', 'being driven', 'driven by', 'by short', 'short and', 'and long', 'long term', 'term optimism', 'optimism for', 'for the', 'the commodity', 'commodity due', 'due to', 'to both', 'both strong', 'strong expected', 'expected demand', 'demand and', 'and supply', 'supply constraints.\\nWith', 'constraints.\\nWith the', 'the full', 'full reopening', 'reopening of', 'of the', 'the US', 'US economy', 'economy on', 'on the', 'the horizon', 'horizon and', 'and President', 'President Joe', \"Joe Biden's\", \"Biden's plan\", 'plan to', 'to invest', 'invest heavily', 'heavily in', \"in America's\", \"America's infrastructure,\", 'infrastructure, as', 'as well', 'well as', \"as China's\", \"China's ongoing\", 'ongoing economic', 'economic recovery,', 'recovery, there', 'there is', 'is reason', 'reason to', 'to believe', 'believe copper', 'copper demand', 'demand will', 'will remain', 'remain high.'], rawFeatures=SparseVector(200, {0: 1.0, 1: 1.0, 2: 1.0, 3: 2.0, 4: 1.0, 6: 1.0, 8: 1.0, 11: 2.0, 13: 1.0, 14: 1.0, 15: 1.0, 21: 2.0, 22: 1.0, 26: 1.0, 27: 1.0, 29: 1.0, 35: 2.0, 38: 1.0, 39: 1.0, 42: 1.0, 43: 1.0, 44: 2.0, 45: 1.0, 47: 4.0, 48: 1.0, 55: 2.0, 56: 1.0, 58: 2.0, 62: 1.0, 63: 1.0, 65: 2.0, 66: 1.0, 67: 2.0, 68: 1.0, 72: 1.0, 75: 1.0, 76: 1.0, 81: 2.0, 82: 1.0, 85: 1.0, 87: 1.0, 88: 2.0, 89: 1.0, 90: 1.0, 91: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 98: 2.0, 102: 1.0, 103: 1.0, 106: 1.0, 108: 2.0, 110: 1.0, 113: 1.0, 114: 1.0, 115: 1.0, 119: 1.0, 124: 2.0, 129: 1.0, 130: 3.0, 132: 1.0, 135: 2.0, 136: 1.0, 140: 1.0, 141: 2.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 149: 1.0, 154: 1.0, 159: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 167: 1.0, 168: 1.0, 170: 2.0, 171: 2.0, 175: 3.0, 178: 1.0, 180: 1.0, 182: 2.0, 183: 2.0, 185: 1.0, 187: 2.0, 190: 1.0, 191: 1.0, 194: 1.0, 197: 2.0}), features=SparseVector(200, {0: 0.4055, 1: 0.6931, 2: 0.6931, 3: 1.3863, 4: 0.4055, 6: 0.1823, 8: 1.0986, 11: 0.8109, 13: 0.6931, 14: 0.6931, 15: 0.4055, 21: 1.3863, 22: 0.4055, 26: 0.4055, 27: 0.4055, 29: 0.4055, 35: 1.3863, 38: 0.4055, 39: 0.1823, 42: 0.6931, 43: 0.4055, 44: 0.3646, 45: 0.4055, 47: 0.7293, 48: 0.4055, 55: 0.8109, 56: 0.1823, 58: 2.1972, 62: 0.6931, 63: 0.1823, 65: 0.8109, 66: 1.0986, 67: 0.8109, 68: 0.1823, 72: 0.6931, 75: 0.1823, 76: 0.4055, 81: 1.3863, 82: 0.6931, 85: 0.0, 87: 0.6931, 88: 0.8109, 89: 0.4055, 90: 0.4055, 91: 0.1823, 94: 0.0, 95: 0.1823, 96: 0.4055, 98: 0.3646, 102: 0.4055, 103: 1.0986, 106: 0.6931, 108: 1.3863, 110: 0.0, 113: 0.6931, 114: 0.6931, 115: 0.1823, 119: 0.4055, 124: 1.3863, 129: 0.6931, 130: 1.2164, 132: 0.4055, 135: 0.8109, 136: 0.4055, 140: 0.1823, 141: 0.8109, 142: 0.4055, 143: 0.6931, 144: 0.6931, 145: 0.1823, 149: 0.1823, 154: 0.4055, 159: 1.0986, 162: 1.0986, 163: 0.4055, 164: 0.1823, 167: 0.0, 168: 0.4055, 170: 0.3646, 171: 1.3863, 175: 0.547, 178: 0.4055, 180: 0.0, 182: 0.3646, 183: 0.3646, 185: 0.4055, 187: 0.8109, 190: 0.1823, 191: 0.4055, 194: 0.4055, 197: 2.1972}))\n",
            "(200,[0,1,2,3,4,6,8,11,13,14,15,21,22,26,27,29,35,38,39,42,43,44,45,47,48,55,56,58,62,63,65,66,67,68,72,75,76,81,82,85,87,88,89,90,91,94,95,96,98,102,103,106,108,110,113,114,115,119,124,129,130,132,135,136,140,141,142,143,144,145,149,154,159,162,163,164,167,168,170,171,175,178,180,182,183,185,187,190,191,194,197],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0])\n",
            "Row(label=0.5, document=['Many', 'of', 'the', 'same', 'theses', 'that', 'bulls', 'have', 'extolled', 'for', 'years', 'continue', 'to', 'drive', \"Bitcoin's\", 'ascent.', 'For', 'example,', 'fans', 'still', 'view', 'its', '21', 'million', 'token', 'limit', 'as', 'a', 'hedge', 'against', 'the', 'long-term', 'devaluation', 'of', 'the', 'U.S.', 'dollar', 'as', 'the', 'money', 'supply', 'increases.\\n\\nEnthusiasts', 'also', 'still', 'value', 'Bitcoin', 'for', 'its', 'payment', 'potential.', 'A', 'greater', 'number', 'of', 'businesses', 'are', 'accepting', 'cryptocurrency', 'now', 'than', 'ever', 'before,', 'and', \"Bitcoin's\", 'network', 'can', 'settle', 'payment', 'transactions', 'in', 'an', 'average', 'of', '10', 'minutes.', 'This', 'compares', 'to', 'cross-border', 'payments', 'on', 'traditional', 'networks,', 'which', 'can', 'take', 'up', 'to', 'a', 'week', 'to', 'be', 'validated', 'and', 'settled.\\n\\nThe', \"world's\", 'most', 'popular', 'digital', 'currency', 'has', 'also', 'received', 'a', 'boost', 'from', 'brand-name', 'enterprise', 'adoption.', 'Electric-vehicle', 'maker', 'Tesla', 'recently', 'announced', 'a', '$1.5', 'billion', 'purchase', 'of', 'Bitcoin,', 'which', 'will', 'be', 'added', 'to', 'the', \"automaker's\", 'balance', 'sheet.\\n\\nBut', 'you', 'might', 'be', 'disappointed', 'if', 'you', 'dig', 'beyond', 'the', 'short-term', 'cheerleading', 'and', 'technical', 'analysis.', 'History', 'is', 'clear', 'that', 'when', 'next-big-thing', 'investments', 'go', 'parabolic,', 'they', 'eventually', 'implode.', 'Furthermore,', 'Bitcoin', 'is', 'full', 'of', 'flaws', 'and', 'misconceptions', 'perpetuated', 'on', 'social', 'media', 'platforms.\\n'], ngrams=['Many of', 'of the', 'the same', 'same theses', 'theses that', 'that bulls', 'bulls have', 'have extolled', 'extolled for', 'for years', 'years continue', 'continue to', 'to drive', \"drive Bitcoin's\", \"Bitcoin's ascent.\", 'ascent. For', 'For example,', 'example, fans', 'fans still', 'still view', 'view its', 'its 21', '21 million', 'million token', 'token limit', 'limit as', 'as a', 'a hedge', 'hedge against', 'against the', 'the long-term', 'long-term devaluation', 'devaluation of', 'of the', 'the U.S.', 'U.S. dollar', 'dollar as', 'as the', 'the money', 'money supply', 'supply increases.\\n\\nEnthusiasts', 'increases.\\n\\nEnthusiasts also', 'also still', 'still value', 'value Bitcoin', 'Bitcoin for', 'for its', 'its payment', 'payment potential.', 'potential. A', 'A greater', 'greater number', 'number of', 'of businesses', 'businesses are', 'are accepting', 'accepting cryptocurrency', 'cryptocurrency now', 'now than', 'than ever', 'ever before,', 'before, and', \"and Bitcoin's\", \"Bitcoin's network\", 'network can', 'can settle', 'settle payment', 'payment transactions', 'transactions in', 'in an', 'an average', 'average of', 'of 10', '10 minutes.', 'minutes. This', 'This compares', 'compares to', 'to cross-border', 'cross-border payments', 'payments on', 'on traditional', 'traditional networks,', 'networks, which', 'which can', 'can take', 'take up', 'up to', 'to a', 'a week', 'week to', 'to be', 'be validated', 'validated and', 'and settled.\\n\\nThe', \"settled.\\n\\nThe world's\", \"world's most\", 'most popular', 'popular digital', 'digital currency', 'currency has', 'has also', 'also received', 'received a', 'a boost', 'boost from', 'from brand-name', 'brand-name enterprise', 'enterprise adoption.', 'adoption. Electric-vehicle', 'Electric-vehicle maker', 'maker Tesla', 'Tesla recently', 'recently announced', 'announced a', 'a $1.5', '$1.5 billion', 'billion purchase', 'purchase of', 'of Bitcoin,', 'Bitcoin, which', 'which will', 'will be', 'be added', 'added to', 'to the', \"the automaker's\", \"automaker's balance\", 'balance sheet.\\n\\nBut', 'sheet.\\n\\nBut you', 'you might', 'might be', 'be disappointed', 'disappointed if', 'if you', 'you dig', 'dig beyond', 'beyond the', 'the short-term', 'short-term cheerleading', 'cheerleading and', 'and technical', 'technical analysis.', 'analysis. History', 'History is', 'is clear', 'clear that', 'that when', 'when next-big-thing', 'next-big-thing investments', 'investments go', 'go parabolic,', 'parabolic, they', 'they eventually', 'eventually implode.', 'implode. Furthermore,', 'Furthermore, Bitcoin', 'Bitcoin is', 'is full', 'full of', 'of flaws', 'flaws and', 'and misconceptions', 'misconceptions perpetuated', 'perpetuated on', 'on social', 'social media', 'media platforms.\\n'], rawFeatures=SparseVector(200, {0: 2.0, 1: 3.0, 2: 1.0, 4: 1.0, 5: 2.0, 6: 2.0, 7: 1.0, 9: 1.0, 10: 1.0, 14: 2.0, 15: 1.0, 16: 2.0, 20: 1.0, 23: 4.0, 24: 1.0, 25: 1.0, 26: 2.0, 27: 2.0, 29: 1.0, 30: 1.0, 31: 1.0, 34: 1.0, 35: 1.0, 41: 1.0, 42: 1.0, 44: 3.0, 45: 2.0, 46: 1.0, 48: 1.0, 49: 2.0, 50: 1.0, 54: 1.0, 55: 2.0, 56: 2.0, 59: 1.0, 60: 1.0, 61: 1.0, 63: 2.0, 67: 3.0, 68: 1.0, 69: 1.0, 71: 1.0, 72: 1.0, 75: 2.0, 76: 1.0, 77: 2.0, 78: 1.0, 80: 1.0, 82: 1.0, 83: 1.0, 85: 2.0, 86: 1.0, 88: 1.0, 90: 1.0, 91: 1.0, 92: 2.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 1.0, 97: 3.0, 98: 1.0, 99: 2.0, 101: 1.0, 102: 1.0, 105: 1.0, 107: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 115: 1.0, 119: 2.0, 120: 2.0, 123: 2.0, 124: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 130: 1.0, 134: 1.0, 136: 2.0, 137: 2.0, 139: 1.0, 141: 1.0, 145: 2.0, 146: 1.0, 147: 2.0, 149: 1.0, 150: 5.0, 151: 2.0, 153: 1.0, 155: 2.0, 156: 1.0, 157: 1.0, 158: 1.0, 161: 1.0, 164: 2.0, 165: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 2.0, 172: 1.0, 174: 1.0, 175: 1.0, 177: 1.0, 178: 1.0, 179: 1.0, 180: 2.0, 181: 1.0, 183: 1.0, 186: 2.0, 188: 2.0, 189: 1.0, 193: 1.0, 195: 1.0, 196: 2.0, 198: 2.0}), features=SparseVector(200, {0: 0.8109, 1: 2.0794, 2: 0.6931, 4: 0.4055, 5: 1.3863, 6: 0.3646, 7: 0.6931, 9: 1.0986, 10: 1.0986, 14: 1.3863, 15: 0.4055, 16: 0.8109, 20: 0.4055, 23: 0.7293, 24: 0.6931, 25: 0.1823, 26: 0.8109, 27: 0.8109, 29: 0.4055, 30: 0.4055, 31: 0.4055, 34: 0.4055, 35: 0.6931, 41: 1.0986, 42: 0.6931, 44: 0.547, 45: 0.8109, 46: 0.6931, 48: 0.4055, 49: 1.3863, 50: 0.4055, 54: 0.6931, 55: 0.8109, 56: 0.3646, 59: 0.1823, 60: 0.4055, 61: 0.6931, 63: 0.3646, 67: 1.2164, 68: 0.1823, 69: 1.0986, 71: 0.4055, 72: 0.6931, 75: 0.3646, 76: 0.4055, 77: 0.8109, 78: 1.0986, 80: 0.4055, 82: 0.6931, 83: 0.4055, 85: 0.0, 86: 0.1823, 88: 0.4055, 90: 0.4055, 91: 0.1823, 92: 2.1972, 93: 0.4055, 94: 0.0, 95: 0.1823, 96: 0.4055, 97: 3.2958, 98: 0.1823, 99: 0.8109, 101: 0.4055, 102: 0.4055, 105: 0.4055, 107: 0.6931, 109: 0.4055, 110: 0.0, 111: 0.6931, 112: 0.4055, 115: 0.1823, 119: 0.8109, 120: 0.3646, 123: 1.3863, 124: 0.6931, 126: 0.4055, 127: 0.6931, 128: 0.4055, 130: 0.4055, 134: 0.6931, 136: 0.8109, 137: 2.1972, 139: 1.0986, 141: 0.4055, 145: 0.3646, 146: 0.4055, 147: 2.1972, 149: 0.1823, 150: 2.0273, 151: 1.3863, 153: 0.4055, 155: 0.8109, 156: 0.6931, 157: 0.6931, 158: 0.6931, 161: 0.6931, 164: 0.3646, 165: 0.4055, 167: 0.0, 168: 0.4055, 169: 1.0986, 170: 0.3646, 172: 1.0986, 174: 0.4055, 175: 0.1823, 177: 0.4055, 178: 0.4055, 179: 0.4055, 180: 0.0, 181: 1.0986, 183: 0.1823, 186: 0.8109, 188: 2.1972, 189: 1.0986, 193: 1.0986, 195: 0.4055, 196: 0.8109, 198: 2.1972}))\n",
            "(200,[0,1,2,4,5,6,7,9,10,14,15,16,20,23,24,25,26,27,29,30,31,34,35,41,42,44,45,46,48,49,50,54,55,56,59,60,61,63,67,68,69,71,72,75,76,77,78,80,82,83,85,86,88,90,91,92,93,94,95,96,97,98,99,101,102,105,107,109,110,111,112,115,119,120,123,124,126,127,128,130,134,136,137,139,141,145,146,147,149,150,151,153,155,156,157,158,161,164,165,167,168,169,170,172,174,175,177,178,179,180,181,183,186,188,189,193,195,196,198],[2.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,4.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,5.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}